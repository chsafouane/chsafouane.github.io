[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog serves for archiving my notes."
  },
  {
    "objectID": "posts/Byte Pair Encoding Tokenization/Byte Pair Encoding Tokenization.html",
    "href": "posts/Byte Pair Encoding Tokenization/Byte Pair Encoding Tokenization.html",
    "title": "Byte Pair Encoding Tokenization",
    "section": "",
    "text": "The aim of this blog is to explain to you how BPE Tokenization works. Weâ€™re going to build a basic tokenizer using BPE tokenization and weâ€™ll apply it on a dummy example.\nWeâ€™re not going to go into the gory details of it. If youâ€™d like a great source for the details of tokenization, watch this 2-hour long video by the amazing Andrej Karpathy. Everything in this post comes from Andrejâ€™s video or the resources he shared in his video.\nTable of contents\n- Hexadecimal representation\n- UTF-8 encoding\n- Some Unicode vocab\n- WTF is UTF-8 ?\n- A sentence example\n- BPE Tokenization\n- BPE in action\n- How does it work\n- Let us build a BPE tokenizer\n- Encoding\n- Decoding\n- One last problem to solve\n- One final example"
  },
  {
    "objectID": "posts/Byte Pair Encoding Tokenization/Byte Pair Encoding Tokenization.html#hexadecimal-representation",
    "href": "posts/Byte Pair Encoding Tokenization/Byte Pair Encoding Tokenization.html#hexadecimal-representation",
    "title": "Byte Pair Encoding Tokenization",
    "section": "Hexadecimal representation",
    "text": "Hexadecimal representation\nLet go for a quick tour of hexadecimal representation. We, as humans, usually represent numbers in base 10. Computers make use of base 2 representation.\nHexadecimal representation means representing a number in base 16. Numbers are represented using 0 to 9 and A-F, with A being equivalent to 10, B to 11, â€¦, F to 15.\nThe number 125 can be written in hexadecimal as 7D.\n\n# d * (16**0) + 7 * (16**1) = 13 + 7 * 16\nhex(125)\n\n'0x7d'\n\n\nNow, a byte is represented with 8 bits, with a range going from 00000000 to 11111111 &lt;-&gt; 0 to 255 in decimal &lt;-&gt; 00 to FF in hexadecimal\n\nprint(hex(255))\n\n0xff\n\n\nNote: The prefix 0x is used in intergers to tell Python to interpret the number as a hexadecimal. For example:\n\n# Python understands that this number is represented in hexadecimal\nhex_number = 0xff\n\nprint(hex_number)\n\n255"
  },
  {
    "objectID": "posts/Byte Pair Encoding Tokenization/Byte Pair Encoding Tokenization.html#utf-8-encoding",
    "href": "posts/Byte Pair Encoding Tokenization/Byte Pair Encoding Tokenization.html#utf-8-encoding",
    "title": "Byte Pair Encoding Tokenization",
    "section": "UTF-8 encoding",
    "text": "UTF-8 encoding\nLittle background about UTF-8 encoding. So, UTF-8 is an encoding that represents Unicode characters using 1 to 4 bytes. So, every string is converted to a series of bytes once encoded using a Unicode encoding (such as UTF-8).\n\nSome Unicode vocab\nYou should know at least these two words when dealing with Unicode: character and code point: - A character is a textual unit in Unicode. It can be anything ranging from a letter, a digit, an emoji, a mathematical symbol, etc.\n\ncharacter = \"A\"\nprint(f\"Character: {character}\")\n\n# First letter of my name in Arabic\n# You might think of it as S\n# Arabic speakers, don't insult me for this analogy \narabic_s_character = \"Øµ\"\nprint(f\"Arabic character: {arabic_s_character}\")\n\nCharacter: A\nArabic character: Øµ\n\n\n\nA code point is a unique number assigned to each character in Unicode. You can obtain it in Python using the ord function.\n\n\nA_code_point = ord(character)\nprint(f\"The character {character} has as a code point {A_code_point} in decimal, which corresponds to {hex(A_code_point)} in hexadecimal\")\n\narabic_s_code_point = ord(arabic_s_character)\nprint(f\"The character {arabic_s_character} has as a code point {arabic_s_code_point} in decimal, which corresponds to {hex(arabic_s_code_point)} in hexadecimal\")\n\nThe character A has as a code point 65 in decimal, which corresponds to 0x41 in hexadecimal\nThe character Øµ has as a code point 1589 in decimal, which corresponds to 0x635 in hexadecimal\n\n\nNice to know: - Use ord to go from a character -&gt; a code point - Use chr to go from a code point -&gt; a character\nAt this step, I highly advise you to take a look at the start of the Unicode Character Table.\nIf you look at this table, youâ€™ll see that the character Øµ doesnâ€™t have 1589 as a code point, it rather has U+0635. These are the same number, itâ€™s just that Unicode uses the hexadecimal notation for code points instead of the decimal one.\n\n\nWTF is UTF-8 ?\nSo, UTF-8 is a Unicode encoding method. It encodes code points in 1 to 4 bytes (it is a variable-length encoding, not all code points are encoded as 4 bytes as this will consume so much memory!).\nOther encoding methods such as UTF-16 and UTF-32 exist but they do things differently than UTF-8. UTF-8 is the most used encoding method in the wild for reasons that I wonâ€™t go into in this post.\nNow, the thing is that UTF-8 encoding doesnâ€™t just convert the Unicode code point to its binary presentation. It does it in a specific way. The following table from wikipedia explains very well the conversion from code points to UTF8-encoding.\nThis table shows four ranges, the 1st range is encoded using 1 byte only, the second one 2 bytes, etc. Each range has its own norm for encoding code points using UTF-8.\n\n\n\nCharacter Øµ example:\nThe character Øµ code point is U+0635 (1589 in decimal). If you try to represent this using binary presentation, it doesnâ€™t fit into 1 byte, youâ€™ll need to use 2 bytes.\nUsing 2 bytes, it yields the following 00000110 00110101. Now, UTF-8 doesnâ€™t use conversion to bytes to encode code points. It has its own rules to do the encoding.\nNow, if you look at the table, you see that U+0635 falls between the range of U+0080 and U+07FF. So, we have to use the rule in the second row to encode it using UTF-8.\nThe way to do this is to fill the x with the numbers in the binary representation starting from the right (copy-paste digit from the binary representation starting from the right): - Before filling: [110]xxxxx [10]xxxxxx - After filling: [110]11000 [10]110101. In hexadecimal, this yields d8 b5.\n\nprint(f\"The byte 11011000 yields in hexadecimal: {hex(0b11011000)}\")\nprint(f\"The byte 10110101 yields in hexadecimal: {hex(0b10110101)}\")\n\nThe byte 11011000 yields in hexadecimal: 0xd8\nThe byte 10110101 yields in hexadecimal: 0xb5\n\n\nNow, this should match the result of the UTF-8 encoding of the character Øµ using Python\n\n\"Øµ\".encode(\"utf-8\")\n\nb'\\xd8\\xb5'\n\n\nThe same rule can be applied to all the 4 ranges in order to go from a code point to its UTF-8 encoding.\nAt this step, I suggest you pick some characters from the unicode table page Iâ€™ve given you above and do the encoding manually.\nNote: There is something that you should note at this step: - Unicode allows us to go from a character to a code point - The UTF-8 encoding (which is one of Unicode available encodings) allows us to go from a code point to a 1 to 4 bytes representation, depending on the character.\nThis yields the following trajectory: Unicode character &lt;-&gt; Code point -&gt; 1 to 4 bytes representation.\nNow, if you take any 1 to 4 bytes number, it doesnâ€™t always map to a code point as it might not respect the rules specified in the wikipedia table above. For example, 11111111 (255 in decimal) corresponds to 1 byte but itâ€™s not a valid unicode encoding. This is because the rule specifies that all 1-byte encoded characters should start with a 0 and not a 1.\n\n\nA sentence example\nLet us now encode a whole sentence and see the result: Hello there ØµÙÙˆØ§Ù†\n\nex_sentence = \"Hello there ØµÙÙˆØ§Ù†\"\n\nprint(f\"The UTF-8 encoding of the sentence '{ex_sentence}' is: {ex_sentence.encode('UTF-8')}\")\n\nThe UTF-8 encoding of the sentence 'Hello there ØµÙÙˆØ§Ù†' is: b'Hello there \\xd8\\xb5\\xd9\\x81\\xd9\\x88\\xd8\\xa7\\xd9\\x86'\n\n\nI hope you at least recognize the UTF-8 encoding of the character Øµ: \\xd8\\xb5\nA question that might come to your mind upong seeing the result is why the hell we have characters like H in a bytes representation. This is something that has to do with the display of bytes in Python.\nPython uses a mixed representation of bytes for readability: - ASCII characters, even when encoded as bytes, are displayed in their readable form - Non-ASCII characters, when encoded as bytes, are displayed in hexadecimal format\nItâ€™s just a displaying matter !"
  },
  {
    "objectID": "posts/Byte Pair Encoding Tokenization/Byte Pair Encoding Tokenization.html#bpe-tokenization",
    "href": "posts/Byte Pair Encoding Tokenization/Byte Pair Encoding Tokenization.html#bpe-tokenization",
    "title": "Byte Pair Encoding Tokenization",
    "section": "BPE Tokenization",
    "text": "BPE Tokenization\nIn order to feed data to a model, we have to first convert it to numerical data. The process for NLP models looks as follow:\n\nText â€”&gt; Tokenizer â€”&gt; Numerical ids (tokens) â€”&gt; NLP model â€”&gt; Prediction\n\nNow, if you look at the definition of str in Python, itâ€™s defined as follows:\nTextual data in Python is handled with str objects, or strings. Strings are immutable sequences of Unicode code points\nFrom this definition, you can decide to just map every unicode character to its code point. For e.g, Øµ will be mapped to 1589. Using this technique will yield a vocabulary of almost 150 000 tokens.\nThe problem with this is: - Your vocabulary will be big. This means that your model will have embedding layers of 150 000 * size_of_your_embeddding - With that many characters, itâ€™s very likely that your model training dataset wonâ€™t contain many of them that often, if not at all. If your model gets to see Øµ very few times, its embedding will be very ill trained. At the end of training, its embedding will be very close to its random initialization state and would be of no use at all. - As your input sentence will be divided in unique characters, a simple sentence will consume lots of tokens in your input context. This means that: - At inference, youâ€™ll have to make predictions for every character - Your sentence will take so much memory because even a simple sentence will be long, many embeddings used and calculations will have to be made over all those embeddings - Simple text will consume so much of the model context size\nLet look at this famous sentence The quick brown fox jumps over the lazy dog. If every character is considered a token, it will consist of 28 different tokens\n\nexample_sentence= \"The quick brown fox jumps over the lazy dog\"\nprint(f\"Number of tokens in {example_sentence} is: {len(set([e for e in example_sentence]))}\")\n\nNumber of tokens in The quick brown fox jumps over the lazy dog is: 28\n\n\nIf we use GPT-4o tokenizer that is a BPE tokenizer itself, let us see how many tokenizer we get:\n\nimport tiktoken\n\ntokenizer = tiktoken.get_encoding(\"o200k_base\")\nencoded_input = tokenizer.encode(example_sentence)\nprint(f\"Number of tokens in {example_sentence} with GPT-4o tokenizer is: {len(encoded_input)}\")\n\nNumber of tokens in The quick brown fox jumps over the lazy dog with GPT-4o tokenizer is: 9\n\n\nWhen we look at the tokens resulting from GPT-4o tokenizer, we get to see that every word is a token in this case.\n\nfor token in encoded_input:\n    print(f\"The token with id {token} represents {tokenizer.decode_single_token_bytes(token)}\")\n\nThe token with id 976 represents b'The'\nThe token with id 4853 represents b' quick'\nThe token with id 19705 represents b' brown'\nThe token with id 68347 represents b' fox'\nThe token with id 65613 represents b' jumps'\nThe token with id 1072 represents b' over'\nThe token with id 290 represents b' the'\nThe token with id 29082 represents b' lazy'\nThe token with id 6446 represents b' dog'\n\n\nIf youâ€™d like to play with different tokenizers, visit this page: https://tiktokenizer.vercel.app/, select the tokenizer and get going!\n\nBPE in action\nFor this, youâ€™ll need to install tiktoken package using pip install tiktoken.\n\nimport tiktoken\n\ntokenizer = tiktoken.get_encoding(\"o200k_base\")\n\n\ninput_str = \"My name is ØµÙÙˆØ§Ù†\"\nencoded_input = tokenizer.encode(input_str)\nprint(f\"The result of tokenization consists of {len(encoded_input)} tokens: {encoded_input}\")\n\nThe result of tokenization consists of 5 tokens: [5444, 1308, 382, 37315, 10878]\n\n\nLet us decode each one of these tokens and see what they consist of:\n\nfor token in encoded_input:\n    print(f\"The token with id {token} represents {tokenizer.decode_single_token_bytes(token)}\")\n\nThe token with id 5444 represents b'My'\nThe token with id 1308 represents b' name'\nThe token with id 382 represents b' is'\nThe token with id 37315 represents b' \\xd8\\xb5\\xd9\\x81'\nThe token with id 10878 represents b'\\xd9\\x88\\xd8\\xa7\\xd9\\x86'\n\n\nI hope you recognize, in the 4th print statement, the hexadecimal representation of the character Øµ We can see that the fourth\n\n[tokenizer.decode_single_token_bytes(x) for x in encoded_input]\n\n[b'My', b' name', b' is', b' \\xd8\\xb5\\xd9\\x81', b'\\xd9\\x88\\xd8\\xa7\\xd9\\x86']\n\n\nLet us decode that fourth element and see what it consists of:\n\ntokenizer.decode_single_token_bytes(encoded_input[3]).decode(\"utf-8\")\n\n' ØµÙ'\n\n\nWe can see that this token consists of 3 unicode characters: - The space character - Øµ, the character with the code point U+0635, whose UTF-8 encoding is d8 b5 - Ù, the character with the code point U, whose UTF-8 encoding is d9 81\n\n\nHow does it work\nAs you might have guessed from its name, the Byte Pair Encoding mechanism does mainly two things to tokenize your data: - It represents strings as streams of bytes. Now, if you represent your data using only bytes, as a character usually consists of many bytes, that will be worse than using unicode code points as tokenized strings will be very very long. A byte can go only from 0 to 255.\n\nWhen some pair of bytes are so common in the corpus, itâ€™s going to merge them going forward and assign a new id to them. So, in the example of the token â€™ ØµÙâ€™, GPT-4 tokenizer has decided to merge the bytes that these two arabic characters consist of, meaning these four bytes:\n\nspace character (20 in hex, 32 in decimal)\nd8 (216 in decimal)\nb5 (181 in decimal)\nd9 (217 in decimal)\n81 (129 in decimal)\n\n\nand assigned to the whole token '  ØµÙ' the id 37315.\nOf course, the vocabulary identified by tokenizers canâ€™t just grow indefinitly. otherwise, your model embedding table will also be huge and the model will be practically unusable. Thatâ€™s why BPE tokenizers have a hyperparameter that specifies the number of merges that can be done when training.\nA quick recap: When training a tokenizer, convert string to bytes. The result will consist of numbers going from 0 to 255. Merge bytes that come-up together so often and assign an id to them.\n\n\nLet us build a BPE tokenizer\n\nEncoding\nSo, we basically need to do three things to build our tokenizer: - Define our basic vocab that weâ€™ll grow as we merge the most common pair of bytes - Convert our corpus to a stream of bytes (just encoding it using UTF-8) - Do the following number_of_merges times: - Get the most common pair of bytes - Merge the two pairs in a new id. Everytime the tokenizer will see the two pairs together, it will map them to the new ID.\nSo, our starting vocab is just the 1 byte interval (0 to 255), and as we merge the most common, weâ€™ll append new ids corresponding to the merged bytes.\n\nvocab = {i:i for i in range(256)}\n\nAs stated before, the first step is to convert our corpus to a stream of bytes by encoding using UTF-8.\nLet us start with a very basic training corpus, it will make testing our functions very easy. Later, weâ€™ll use a corpus consisting of many languages.\n\ncorpus = \"Hello Hello Hello my name is Safouane and I am the author of this post\"\n\n\nencoded_corpus = corpus.encode(\"UTF-8\")\n\nAs we want to use bytes using the decimal representation (0-255) instead of hexadecimal (0x00 - 0xFF) (the default after encoding with UTF-8) for readability purposes, weâ€™ll do the conversion right away\n\nencoded_corpus = [int(x) for x in encoded_corpus]\nprint(encoded_corpus[:10])\n\n[72, 101, 108, 108, 111, 32, 72, 101, 108, 108]\n\n\nLet build a function that returns the most common pair of bytes in the whole corpus\n\nfrom collections import Counter\n\ndef get_most_common_pair(encoded_corpus: list[int]) -&gt; tuple:\n    \"\"\"Returns a tuple of the most common pair of bytes in a corpus.\n    \"\"\"\n    most_common_pair_and_count = Counter(zip(encoded_corpus[:-2], encoded_corpus[1:])).most_common(1)\n    most_common_pair = most_common_pair_and_count[0][0]\n    return most_common_pair\n\nLet us see whatâ€™s the most common pair of bytes in this corpus\n\nmost_common_pair = get_most_common_pair(encoded_corpus)\n\nprint(f\"The most common pair is {most_common_pair}. {most_common_pair[0]} corresponds to {chr(most_common_pair[0])} and {most_common_pair[1]} corresponds to {chr(most_common_pair[1])}\")\n\nThe most common pair is (72, 101). 72 corresponds to H and 101 corresponds to e\n\n\nLet us write a function that takes the encoded corpus, the vocabulary, the pair of ids to merge and: - Updates the vocabulary by adding the new id that replaces the occurence of the pair of the ids to merge - Updates the corpus by replacing the occurence of the pair of ids with the new id\n\ndef merge_and_update_corpus_and_vocab(encoded_corpus, vocab, pair_to_merge):\n    \"\"\"Updates the vocab with the new pair of ids to merge\n    & updates the corpus to use the new id instead of the pair of bytes\n    \"\"\"\n    updated_corpus = []\n    updated_vocab = vocab.copy()\n\n    # Add id for merged pair in vocab\n    pair_new_id = max(vocab.keys()) + 1\n    updated_vocab[pair_new_id] = pair_to_merge\n\n    # Update corpus to use the new id instead of the couple the pair of ids\n    i = 0\n\n    while i &lt; len(encoded_corpus):\n        if (i &lt; len(encoded_corpus) - 1) and encoded_corpus[i] == pair_to_merge[0] and encoded_corpus[i + 1] == pair_to_merge[1]:\n            updated_corpus.append(pair_new_id)\n            i += 2\n        else:\n            updated_corpus.append(encoded_corpus[i])\n            i +=1\n    \n    return updated_corpus, updated_vocab\n\nLet us merge now the most common pair of bytes and create one id out of it. The successive 101,32 will be turned into 256.\n\nupdated_corpus_ex, updated_vocab_example = merge_and_update_corpus_and_vocab(\n    encoded_corpus, \n    vocab,\n    get_most_common_pair(encoded_corpus)    \n)\n\nWe can see the presence of a new token id in the corpus: 256\n\nprint(updated_corpus_ex[:10])\n\n[256, 108, 108, 111, 32, 256, 108, 108, 111, 32]\n\n\nWhen we check the vocab to see to what it corresponds, we see that itâ€™s a merge of two ids: 72 and 101.\n\nprint(updated_vocab_example[256])\n\n(72, 101)\n\n\nWeâ€™ve seen how merging works. Now, we have to apply this merging a number_of_merges times. What you should keep in mind is the following: - You vocab will grow with the number of merges you apply. Say you apply 10 merges, your vocab will go from 256 ids to 266 ids. - The bigger your vocab gets, the bigger your embedding table (whose size corresponds to number of tokens in the vocab * size of embedding) will grow. - Merging indefinitly is not a good idea. The different ids in your vocab will not come up often as tokens in the training corpus of your model. Consequently, some tokens will have random embeddings - Having merged tokens allows you to make good use of your context size as your tokens will can even represent complete words at times\n\ndef merge_iteratively(corpus, vocab, number_of_merges: int):\n    for _ in range(number_of_merges):\n        most_common_pair = get_most_common_pair(corpus)\n        corpus, vocab = merge_and_update_corpus_and_vocab(corpus, vocab, most_common_pair)\n    \n    return corpus, vocab\n\n\nthree_merges_corpus, three_merges_vocab = merge_iteratively(encoded_corpus, vocab, 3)\n\nprint(f\"A glimpse at the start of new corpus: {three_merges_corpus[:10]}\")\n\nprint(f\"The merged ids: {[three_merges_vocab[255 + x] for x in range(1, 4)]}\")\n\nA glimpse at the start of new corpus: [258, 111, 32, 258, 111, 32, 258, 111, 32, 109]\nThe merged ids: [(72, 101), (256, 108), (257, 108)]\n\n\nNow, the merge_iteratively function if itâ€™s out of pairs that occur more than once, it will start merging pairs that occur only once. This is not a desired behavior.\nThis can be handled easily by returning the count of the number of occurences as well in get_most_common_pair and checking that itâ€™s greater than 1 to proceed to merging.\n\ndef get_most_common_pair(encoded_corpus: list[int]) -&gt; tuple:\n    \"\"\"Returns a tuple:\n    - A tuple of the most common pair of ids in a corpus\n    - The number of occurences of the pair of ids\n    \"\"\"\n    most_common_pair_and_count = Counter(zip(encoded_corpus[:-2], encoded_corpus[1:])).most_common(1)[0]\n    return most_common_pair_and_count\n\n\ndef merge_iteratively(corpus, vocab, number_of_merges: int):\n    for _ in range(number_of_merges):\n        most_common_pair, count = get_most_common_pair(corpus)\n        if count == 1:\n            break\n        else:\n            corpus, vocab = merge_and_update_corpus_and_vocab(corpus, vocab, most_common_pair)\n    \n    return corpus, vocab\n\nLet put all these functions in one function that should do the encoding of a string\n\ndef encode(corpus: str, num_merges: int):\n    encoded_corpus = corpus.encode(\"UTF-8\")\n    base_vocab = {i:i for i in range(256)}\n    updated_corpus, updated_vocab = merge_iteratively(encoded_corpus, base_vocab, num_merges)\n    return updated_corpus, updated_vocab\n\nNow that we can train a tokenizer and encode a string, all thatâ€™s left is decoding.\n\n\nDecoding\nNow, to detokenize tokenized text, itâ€™s not that hard - First, you have to reverse the mapping. You have to unpair the ids. If for example, the id 257=(100,256) and 256=(255, 20), we have to unpair those. But in order for the unpairing to work, we have to unpair the ids starting from the highest id, 257 in this case. 257 will be replaced with (100 and 256) and then 256 can be replaced with (255 and 20). If you donâ€™t do it in the right order, youâ€™ll have some composed ids left and you wonâ€™t be able to decode them. - Once the mapping is reversed, you can decode the stream of bytes using UTF-8\n\nthree_merges_corpus, three_merges_vocab = encode(corpus, 3)\n\nLet us first build a function that will help us get the pair of ids that were replaced with a new id\n\ndef unpair_ids(corpus, vocab):\n    \"\"\"Reverses new ids in the vocab to their\n    orignal pair of ids.\n    \"\"\"\n    for token in sorted(vocab.keys(), reverse=True):\n        if token &gt; 255:\n            unpaired_corpus = []\n            for i in range(len(corpus)):\n                if corpus[i] == token:\n                    unpaired_corpus.append(vocab[token][0])\n                    unpaired_corpus.append(vocab[token][1])\n                else:\n                    unpaired_corpus.append(corpus[i])\n                \n            corpus = list(unpaired_corpus)\n    \n    return unpaired_corpus\n\n\nprint(f\"The first ten tokens of the tokenized corpus looks like: {three_merges_corpus[:10]}\")\nprint(f\"The first ten tokens of the unpaired corpus looks as follows: {unpair_ids(three_merges_corpus[:10], three_merges_vocab)}\")\n\nThe first ten tokens of the tokenized corpus looks like: [258, 111, 32, 258, 111, 32, 258, 111, 32, 109]\nThe first ten tokens of the unpaired corpus looks like: [72, 101, 108, 108, 111, 32, 72, 101, 108, 108, 111, 32, 72, 101, 108, 108, 111, 32, 109]\n\n\nNow that we recovered the corpus with the bytes ids, we can do the UTF-8 decoding:\n\ndef decode(tokenized_corpus_with_merges, vocab_with_merges):\n    unpaired_tokenized_corpus = unpair_ids(tokenized_corpus_with_merges, vocab_with_merges)\n    # The UTF-8 decoding expects byte objects and not int, we have to convert them to byte first\n    decoded_corpus = b\"\".join([x.to_bytes() for x in unpaired_tokenized_corpus]).decode(\"UTF-8\")\n    return decoded_corpus\n\n\ndecoded_corpus = decode(three_merges_corpus, three_merges_vocab)\nprint(f\"The decoding of the corpus yields: {decoded_corpus}\")\nprint(f\"Check of equality with the original corpus: {decoded_corpus == corpus}\")\n\nThe decoding of the corpus yields: Hello Hello Hello my name is Safouane and I am the author of this post\nCheck of equality with the original corpus: True\n\n\n\n\nOne last problem to solve\nWhen encoding, we get to add new ids as tokens and we just incremented with 1 each time. Now, remember this table from wikipedia:\n\n\n\nWhat this table entails is that not all bytes are valid UTF-8 encodings. For example, any 1-byte number starting with 1 its binary representation is not a valid UTF-8 encoding. The number 10000000 (which corresponds to 128 in the decimal representation) is not a valid UTF-8 encoding.\nLet us check what Python will say if we try to decode it using UTF-8\n\n# Convert the int to a bytes object\nexample_byte = 0b10000000.to_bytes()\n\n# Decode it using UTF-8\nexample_byte.decode(\"UTF-8\")\n\n\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\nCell In[215], line 5\n      2 example_byte = 0b10000000.to_bytes()\n      4 # Decode it using UTF-8\n----&gt; 5 example_byte.decode(\"UTF-8\")\n\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n\n\n\nThe error says invalid start byte, canâ€™t be any clearer !\nNow, what should we do if we encounter such integers in our encoded corpus ? The way this is circumvented in GPT-4 tokenizer for example is by decoding that integer into a specific character ï¿½. This is the default value that any invalid integer is decoded into with decode method using UTF-8 encoding in Python.\n\nexample_byte.decode(\"UTF-8\", errors=\"replace\")\n\n'ï¿½'\n\n\n\n\nOne final example\nAs an example of our training corpus, weâ€™ll take the description in english, french and arabic of 1337, one of the leading IT schools in the world that is located in Morocco. This description will constitute our corpus that weâ€™ll use to train our tokenizer.\n\ninput_string = \"\"\"1337 is the first to provide IT training in Morocco, completely free of charge, open and accessible to anyone between the ages of 18 and 30. No need for an IT degree, or of having undergone extensive IT training. The only criteria for admission in Treize, Trente-Sept is CREATIVITY.\n\nThe Treize, Trente-Sept educational approach is based on peer-learning. A participatory operating style allowing students to unleash their creativity through project-based learning. To train the future coders of tomorrow, we had to rethink learning. We had to make IT something fun, exciting and at odds with the restrictive vision that the general public may have about it.\n\n1337 is the coding school par excellence, completely free of charge and accessible to all with no prerequisite of a degree. It offers a full immersion in a universe where the future is already present. Where IT and the lines of code are way more than a vague and off-putting conceptâ€¦\n\nTreize, Trente-Sept, a forward-looking school from the present.\n1337, câ€™est la premiÃ¨re formation en informatique par excellence au Maroc, entiÃ¨rement gratuite, ouverte 24h/24 7j/7 et accessible Ã  tous sans prÃ©-requis de diplÃ´me, ou de connaissance en informatique.. Câ€™est une immersion complÃ¨te dans un univers oÃ¹ le futur est dÃ©jÃ  prÃ©sent, oÃ¹ lâ€™informatique et les lignes de codes sont plus quâ€™un concept flou et rÃ©barbatifâ€¦\n\nLa pÃ©dagogie de Treize, Trente-Sept sâ€™articule autour du peer-learning. Un fonctionnement participatif qui permet aux Ã©tudiants de libÃ©rer leur crÃ©ativitÃ© grÃ¢ce Ã  lâ€™apprentissage par projet. Pour former les futurs codeurs de demain, il fallait repenser lâ€™apprentissage, faire de lâ€™informatique quelque chose de ludique, de passionnant et aux antipodes de la vision restrictive que le grand public peut en avoir.\n\nTreize, Trente-Sept utilise les mÃ©thodes techniques et pÃ©dagogiques de 42 Paris, Ã©lue meilleure Ã©cole de code au monde par CodinGame.\n\nTreize, Trente-Sept, une Ã©cole du prÃ©sent tournÃ©e vers le futur.\n1337 Ù‡ÙŠ Ø£ÙˆÙ„ ØªÙƒÙˆÙŠÙ† Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙŠ ÙØ§Ù„Ù…ØºØ±Ø¨ØŒ ÙƒÙ„Ùˆ Ø¨Ø§Ù„Ù…Ø¬Ø§Ù†ØŒ Ù…ÙØªÙˆØ­ Ù„Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù„ÙŠ ØªÙŠØªØ±Ø§ÙˆØ­ Ø¹Ù…Ø±Ù‡Ù… Ø¨ÙŠÙ† 18 Ùˆ30 Ø³Ù†Ø©. Ù…Ø§ Ù…Ø­ØªØ§Ø¬Ø´ ÙŠÙƒÙˆÙ† Ø¹Ù†Ø¯Ùƒ Ø¯Ø¨Ù„ÙˆÙ… ÙØ§Ù„Ù…Ø¹Ù„ÙˆÙ…ÙŠØ§ØªØŒ Ø£Ùˆ ØªÙƒÙˆÙ† Ø¯Ø±ØªÙŠ Ø´ÙŠ ØªÙƒÙˆÙŠÙ† Ø£Ùˆ ØªØ®ØµØµ ÙØ§Ù„Ù…Ø¹Ù„ÙˆÙ…ÙŠØ§Øª. Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ù„ÙŠ ÙƒÙŠØªÙŠØ­ Ù„Ùƒ Ø¨Ø§Ø¨ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ù„ 1337 Ù‡Ùˆ Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹.\n\nØ§Ù„Ø¨ÙŠØ¯Ø§ØºÙˆØ¬ÙŠØ© Ø¯ÙŠØ§Ù„ 1337 ÙƒØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ peer-learning Ø§Ù„Ù„ÙŠ Ù‡Ùˆ Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªØ¹Ø§ÙˆÙ†ÙŠ Ø§Ù„Ù„ÙŠ ÙƒÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ø·Ù„Ø¨Ø© Ø¹Ù„Ù‰ ØªØ­Ø±ÙŠØ± Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ Ø¯ÙŠØ§Ù„Ù‡Ù… Ø¨ÙØ¶Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø¨Ø¥Ù†Ø¬Ø§Ø² Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹. ÙˆØ¨Ø§Ø´ Ù†ÙƒÙˆÙ‘Ù†Ùˆ Ù…Ø¨Ø±Ù…Ø¬ÙŠ Ø§Ù„ØºØ¯ØŒ Ø§Ù„Ù„ÙŠ ÙƒÙŠÙƒÙˆØ¯ÙŠÙˆØŒ ÙƒØ§Ù† Ù„Ø§Ø²Ù… Ù†Ø±Ø§Ø¬Ø¹Ùˆ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ¹Ù„Ù…ØŒ ÙˆÙ†Ø¬Ø¹Ù„Ùˆ ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…ÙŠØ§Øª Ø¹Ù…Ù„ÙŠØ© ØªØ±ÙÙŠÙ‡ÙŠØ©ØŒ ÙÙŠÙ‡Ø§ Ø§Ù„Ø±ØºØ¨Ø© ÙˆØ§Ù„Ø´ØºÙØŒ Ù…Ø§Ø´ÙŠ ÙƒÙŠÙ ÙƒÙŠØªØ®ÙŠÙ„ÙˆÙ‡Ø§ Ø§Ù„Ù†Ø§Ø³.\n\n1337 Ù‡ÙŠ Ù…Ø¯Ø±Ø³Ø© Ø§Ù„ÙƒÙˆØ¯ Ø¨Ø§Ù…ØªÙŠØ§Ø²ØŒ Ù…Ø¬Ø§Ù†ÙŠØ© ÙˆÙÙ…ØªÙ†Ø§ÙˆÙ„ Ø§Ù„Ø¬Ù…ÙŠØ¹, ÙˆÙ…Ø§ ØªØªØ·Ù„Ø¨Ø´ Ù…Ù†Ùƒ ØªÙƒÙˆÙ† Ø­Ø§Ø¦Ø² Ø¹Ù„Ù‰ Ø¯Ø¨Ù„ÙˆÙ…. ÙˆØªØ¹ØªØ¨Ø± Ø§Ù†Ø¯Ù…Ø§Ø¬ ÙƒØ§Ù…Ù„ØŒ ÙØ¹Ø§Ù„Ù… Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ Ø§Ù„Ù„ÙŠ Ù…Ø§ Ø¨Ù‚Ø§Ø´ ÙÙŠÙ‡ Ø§Ù„ÙƒÙˆØ¯ Ù…ÙÙ‡ÙˆÙ… ØºØ§Ù…Ø¶.\n\n1337 Ù…Ø¯Ø±Ø³Ø© Ø§Ù„Ø­Ø§Ø¶Ø± Ø§Ù„Ù…ØªØ·Ù„Ø¹Ø© Ù„Ù„Ù…Ø³ØªÙ‚Ø¨Ù„.\n\"\"\"\n\n\nencoded_merged_input, updated_vocab = encode(input_string, 20)\n\nThe added ids to the corpus are the following:\n\nprint({x:updated_vocab.get(x) for x in range(256, max(updated_vocab.keys()) + 1)})\n\n{256: (101, 32), 257: (217, 132), 258: (216, 167), 259: (217, 133), 260: (217, 138), 261: (217, 136), 262: (32, 216), 263: (258, 257), 264: (114, 101), 265: (115, 32), 266: (105, 110), 267: (116, 32), 268: (32, 263), 269: (216, 170), 270: (44, 32), 271: (217, 134), 272: (216, 185), 273: (116, 105), 274: (217, 131), 275: (101, 114)}\n\n\n\ndecoded_corpus = decode(encoded_merged_input, updated_vocab)\n\nLet us check the decoded corpus against the original corpus:\n\ndecoded_corpus == corpus\n\nTrue\n\n\nWhile in this case the result of the decoding process yields the same result as the original, you shouldnâ€™t always expect it to be the case. If during the decoding process, we encounter an id (like 128) that we canâ€™t decode using UTF-8, it will get replaced with ï¿½ in the current implementation. The comparison of the decoding result with the original text wonâ€™t yield equality."
  },
  {
    "objectID": "posts/PDF parsing is hard/PDF parsing is hard.html",
    "href": "posts/PDF parsing is hard/PDF parsing is hard.html",
    "title": "PDF parsing is hard",
    "section": "",
    "text": "The goal of this blogpost is to explain what a PDF is internally and why parsing PDF files is not that easy.\nLately, Iâ€™ve been working on a project with a customer where the goal is to extract some specific information from PDF documents. To my big surprise, this task has proven to be quite challenging.\nThe main challenges stem from the following elements:"
  },
  {
    "objectID": "posts/PDF parsing is hard/PDF parsing is hard.html#what-the-hell-is-a-pdf-document",
    "href": "posts/PDF parsing is hard/PDF parsing is hard.html#what-the-hell-is-a-pdf-document",
    "title": "PDF parsing is hard",
    "section": "What the hell is a PDF document ?",
    "text": "What the hell is a PDF document ?\n\nA first glimpse at a PDF file internal structure\nHave you ever opened a PDF document with notepad or vscode instead of your preferred PDF reader ? If you do so , youâ€™ll stumble upon something that looks like this :\n\n\n\nFigure 1: Internal structure of a PDF document when viewed as raw text\n\n\nIf youâ€™d like to see the full PDF internal structure, you can find the example PDF here.\n\n\nPDF Page Description Language\nTo understand why PDFs are hard to parse, one must understand how a PDF file is built.\nA PDF file is based on Page Description Language (PDL), which is a language used to describe the layout and appearance of a printed page. PDF PDL provides a standardized set of commands to reconstruct a page with perfect fidelity.\nAs a result, a PDF file is essentially a collection of instructions for rendering a page, rather than a linear sequence of text and images. If you look at the example pdf available in the github gist, youâ€™ll see starting line 34 the following commands:\n/F1 18 Tf\n100 700 Td\n(This is a PDF tutorial) Tj\nWhat the following instructions do is:\n\n/F1 18 Tf : set the font to F1 with size 18\n100 700 Td : move the text position to (100, 700)\n(This is a PDF tutorial) Tj : show the text string\n\nEvery PDF looks just like this; a precise sequence of commands that specify what to draw and exactly at what coordinates. It does not contain a semantic representation of its content. It does not state, â€œThis is a paragraph that flows through two columnsâ€ or â€œthis is a tableâ€.\nA table, for example, is just a grid of lines and text positioned at specific coordinates. There are no inherent relationships between the cells, no indication of headers or footers, and no understanding of the data contained within.\nSo when a parser sees whatâ€™s supposed to be a table, it sees just a bunch of lines and text. Its task (rather difficult task) is to infer the structure and relationships between these elements.\nThis lack of semantic structure makes it challenging to parse complex PDF documents.\n\n\nThe internal structure of a PDF\nThis is an optional part and will not help you in understanding the difficulty of parsing PDFs. So you can skip if you want to focus on the practical aspects of PDF parsing.\nWhat you see in Figure 1 or in the gist file is the internal structure of a PDF document. Let us dive into the key components that make up this structure.\nA PDF is composed internally of four sections:\n\n\n\nFigure 2: Internal structure of a PDF document\n\n\nSource: ResearchGate - An example of the PDF file structure\n\n\n\nThe header\nThe header of a PDF file tells you about the PDF specifications version used to generate it. It is always the first line of the file and starts with the %PDF- marker. In Figure 1, it corresponds to %PDF-1.7.\n\n\nThe body\nNow, the body is where you define the content of the PDF. Everything that you see when you open a PDF - the text you read, the images you view, the fonts that make the text look pretty - all of this is generated from the â€œobjectsâ€ defined in the body.\nThink about when you open a PDF and scroll through it. You might see different pages, each with its own layout, fonts, and content. Behind the scenes, each of these elements is stored as a separate object in the PDF body.\nIf you look at the PDF file in the gist, youâ€™ll see that it contains 5 objects. Each object in a PDF is identified by a unique object number and a generation number (usually 0 for new objects).\n\n\nThe cross-reference table\nNow, hereâ€™s where the magic happens that makes PDFs so fast to navigate. You know how you can instantly jump to page 50 of a 200-page PDF, or how quickly a PDF opens even when itâ€™s a large file? Thatâ€™s thanks to the cross-reference table (or â€œxref tableâ€).\nThe xref table maps object numbers to their byte positions in the file. Hereâ€™s the one from our sample PDF:\nxref\n0 6\n0000000000 65535 f \n0000000017 00000 n \n0000000070 00000 n \n0000000126 00000 n \n0000000281 00000 n \n0000000385 00000 n \nLet me break this down:\n\nxref marks the start of the cross-reference table\n0 6 means this section covers 6 objects starting from object 0\nEach line has three parts: byte_offset generation_number n/f\n\nByte offset: The exact position in the file where the object starts (like a street address)\nGeneration number: Usually 0 for active objects\nn/f flag: n means the object is in use, f means itâ€™s free (deleted)\n\n\nThe generation number and the n/f flag are useful when the PDF is modified.\nFor example, when you want to see the page content, the PDF reader looks up object 4 (content stream) in this table, sees it at byte position 281, jumps directly there, and gets the drawing commands. No searching, no scanning, just instant access.\nThis is why PDFs load so quickly even when theyâ€™re huge files. Your PDF viewer doesnâ€™t have to read the whole document.\n\n\nThe trailer\nFinally, we have the trailer - think of it as the PDFâ€™s â€œinstruction manualâ€ that tells your PDF viewer how to get started. When you double-click a PDF file to open it, your PDF reader doesnâ€™t start reading from the beginning. Instead, it jumps to the end of the file and reads the trailer first.\nHereâ€™s the trailer from our sample PDF:\ntrailer\n&lt;&lt; /Size 6\n/Root 1 0 R\n&gt;&gt;\nstartxref\n449\n%%EOF\nHereâ€™s what happens when you open a PDF:\n\n/Size 6: The PDF reader learns there are 6 objects total in the cross-reference table\n/Root 1 0 R: â€œStart reading from the catalog object (object 1) to understand the document structureâ€\nstartxref 449: â€œThe cross-reference table starts at byte position 449â€\n%%EOF: â€œThis is truly the end of the fileâ€\n\nSo when you open a PDF, hereâ€™s the process: 1. Your PDF reader jumps to the end and reads the trailer 2. From the trailer, it gets to know that the xref table is at position 449 and reads it to understand where all objects are 3. When you scroll or jump to a page, it uses the xref table to instantly find the right objects\nThis four-part structure (header, body, xref, trailer) is what makes PDFs very fast to read no matter their size."
  },
  {
    "objectID": "posts/PDF parsing is hard/PDF parsing is hard.html#docling-v2",
    "href": "posts/PDF parsing is hard/PDF parsing is hard.html#docling-v2",
    "title": "PDF parsing is hard",
    "section": "Docling V2",
    "text": "Docling V2\n\n\n\n\nProject context\nSo, here are the main constraints of the project: - It had some high privacy concerns. This means that using an external API for PDF parsing was not an option. - The PDFs were very complex. They contained a mix of text and really complex tables, math formulas, etc. - The customer preferred to run the workload of PDF parsing on their on-premise infrastructure that didnâ€™t provide any GPU.\nThe point that was to our advantage is that the incoming documents didnâ€™t need to be processed online. Batch processing of documents was sufficient, and I could afford to take our time with the parsing.\n\n\nWhy I went for Docling ?\n\nThe fast & good enough didnâ€™t cut it!\nWhile I used to go with PyMuPDF for PDF parsing because itâ€™s easy to use, fast, and good enough for PDF with text and simple tables, it did a really bad job on the documents that I had to handle. The tables were very badly parsed.\nTo preserve the speed advantage, I decided to use a combination of PyMuPDF for the initial text extraction and a library dedicated to table extraction like tabula or camelot. Not only did this approach add more dependencies and more complexity, tabula and camelot both did a very bad job at extracting most tables.\nNow, these packages look at the PDF instructions (like the ones shown in the PDL section) and try to reverse-engineer the layout of the document. While this makes for fast parsing, they have a hard time parsing a two-column layout accurately or a complex table. A good way to put it is that they have a hard time looking at things a human does.\ncamelot and tabula both rely on heuristics and rules that work well for simple well-structured tables.\n\n\nML-focused parsing\nML-focused parsing usually make use of: - a parsing engine to parse text - Some layout detection model to detect the different object in the page (e.g.Â headers, text blocks, images, tables, etc.) - Some table analysis model - Some post-processing steps to combine all the results together\nI benchmarked unstructured and docling and found that docling was doing a better job at parsing the documents I had to handle.\nNote: I didnâ€™t test unstructured API which seems to offer more functionalities than the open-source package.\n\n\n\nHow does Docling work ?\nDocling uses a combination of techniques to parse PDF documents effectively:\n\nText Extraction: Initially, it extracts text from the PDF using a fast and efficient text extraction engine.\nLayout Detection: It employs a layout detection model to identify different elements on the page, such as titles, text, images, tables, etc.\nTable Analysis: For detected tables, Docling uses a specialized table analysis model to understand the structure and content of the table cells.\nOCR: Optionally, one can activate OCR if handling scanned documents.\nAssembly: Finally, it applies post-processing steps to combine all the extracted information and present it in a structured format in DoclingDocument.\n\nThe resulting object DoclingDocument is not a simple text string. It is a hierarchical representation of the original document, containing organized lists of items defined in Docling (TextItem, PictureItem, etc.). Each of these objects holds its content, coordinates, type, and relationships to other elements.\nPlease refer to the Docling documentation for more details on the structure and usage of DoclingDocument.\n\n\nParsing output\nOn top of the docling document that contains anything you can imagine, you can convert the docling document to a a simple string or to a markdown. You can also extract tables as pandas dataframes wich is really very handy.\n\n\nDocling in practice\nI used docling on french documents. A document made of more or less 60 pages, between 5 or 10 tables takes between 60 and 90 seconds using Docling.\nThe things that didnâ€™t work that well include: - tables that roll over two pages. In my experience, Docling considers them as separate tables - multicolumn text is rarely but sometimes is considered a table\n\n\nExplore more with Docling!\n\nDocling can be used to parse many types of documents like powerpoint docs, word docs, html docs, PDFs and more.\n\n\n\n\n - Docling is also designed to be used seamlessly with RAG and supports functionalities like chunking documents. - More recent packages in the docling family of packages are:\n\ndocling-serve: The FastAPI wrappers for running Docling as REST API and distribute large jobs.\ndocling-sdg: Synthetic data generation (SDG) on documents for dataset generation for RAG, finetuning, etc.\ndocling-mcp: The definition of tools with the Model Context Protocol for document conversion, manipulation and generation agents.\n\nWhile I didnâ€™t try these three packages, Iâ€™m looking forward to it.\n\n\nSome final feedback\nWhile docling is an amazing framework, it still has some limitations and areas for improvement. - If youâ€™re just looking for speed and good enough result, go for PyMuPDF - If youâ€™re looking for excellent parsing quality and are ready to trade speed for it, go for Docling\n Happy parsing with DOCLINGÂ !"
  },
  {
    "objectID": "posts/LLM evaluation/LLM evaluation.html",
    "href": "posts/LLM evaluation/LLM evaluation.html",
    "title": "Stop Vibe-Checking: Real-World Lessons on LLM Evals",
    "section": "",
    "text": "Deploying LLM-powered systems in production is the easy part. The hard part? Making sure theyâ€™re actually working.\nIâ€™ve been deploying LLM-powered systems in production in many companies and across different industries for almost 3 years now. Each time, I encountered the same critical challenge: how do you truly evaluate whether your LLM is performing well and not only rely on vibe checks ?\nIf youâ€™re struggling to move beyond â€œit looks good to meâ€ when evaluating your LLM applications, this blog post is for you.\n\n\n\n\n\nThis blog post contains lessons learnt through hands-on experience. These lessons come mostly from my experience testing what I have learnt in deploying real-life LLM applications, talking with peers, doing courses, and reading blog posts.\nApart from my personal experience, Hamel Husain & Shreya Shankar both course & blogs on LLM evaluation have been of a great help to me and many of the techniques I discuss here are either directly quoted from their work or highly inspired by it.\nEach of the lessons below will tackle a specific part of building and evaluating real LLM applications."
  },
  {
    "objectID": "posts/LLM evaluation/LLM evaluation.html#introduction",
    "href": "posts/LLM evaluation/LLM evaluation.html#introduction",
    "title": "Stop Vibe-Checking: Real-World Lessons on LLM Evals",
    "section": "",
    "text": "Deploying LLM-powered systems in production is the easy part. The hard part? Making sure theyâ€™re actually working.\nIâ€™ve been deploying LLM-powered systems in production in many companies and across different industries for almost 3 years now. Each time, I encountered the same critical challenge: how do you truly evaluate whether your LLM is performing well and not only rely on vibe checks ?\nIf youâ€™re struggling to move beyond â€œit looks good to meâ€ when evaluating your LLM applications, this blog post is for you.\n\n\n\n\n\nThis blog post contains lessons learnt through hands-on experience. These lessons come mostly from my experience testing what I have learnt in deploying real-life LLM applications, talking with peers, doing courses, and reading blog posts.\nApart from my personal experience, Hamel Husain & Shreya Shankar both course & blogs on LLM evaluation have been of a great help to me and many of the techniques I discuss here are either directly quoted from their work or highly inspired by it.\nEach of the lessons below will tackle a specific part of building and evaluating real LLM applications."
  },
  {
    "objectID": "posts/LLM evaluation/LLM evaluation.html#sub-lesson-1-real-data-is-better-than-synthetic-data",
    "href": "posts/LLM evaluation/LLM evaluation.html#sub-lesson-1-real-data-is-better-than-synthetic-data",
    "title": "Stop Vibe-Checking: Real-World Lessons on LLM Evals",
    "section": "Sub-lesson 1: Real data is better than synthetic data",
    "text": "Sub-lesson 1: Real data is better than synthetic data\nYou can almost always get some pseudo-real data. If you canâ€™t have access to some beta users, ask your teammates to test the system. They will have very probably some biases of their own, but at least you will get some data that is not completely synthetic and that has different characteristics because itâ€™s coming from different people."
  },
  {
    "objectID": "posts/LLM evaluation/LLM evaluation.html#sub-lesson-2-the-bad-way-to-create-synthetic-data",
    "href": "posts/LLM evaluation/LLM evaluation.html#sub-lesson-2-the-bad-way-to-create-synthetic-data",
    "title": "Stop Vibe-Checking: Real-World Lessons on LLM Evals",
    "section": "Sub-lesson 2: The bad way to create synthetic data",
    "text": "Sub-lesson 2: The bad way to create synthetic data\nReal data is always better than synthetic data. But hey, if you really canâ€™t have some real data, then synthetic data is the way to go.\nThe mistake most people do when creating synthetic data is to ask an LLM to generate queries that are similar to what they expect the users to enter. This is a also a bad idea as the generated queries will be biased towards what you think the users will enter and will likely miss many failure modes. Most importantly, the generated queries will likely be â€œtoo goodâ€ and not representative of real user queries (messy, mispellings, incompleteâ€¦).\nIâ€™ve trained a retriever in the past on synthetic data generated this way. While the performance on synthetic-queries-like was really good, the performance on real user queries was really bad. The gap between synthetic data and real user data was just too big."
  },
  {
    "objectID": "posts/LLM evaluation/LLM evaluation.html#sub-lesson-3-the-good-way-to-create-synthetic-data",
    "href": "posts/LLM evaluation/LLM evaluation.html#sub-lesson-3-the-good-way-to-create-synthetic-data",
    "title": "Stop Vibe-Checking: Real-World Lessons on LLM Evals",
    "section": "Sub-lesson 3: The good way to create synthetic data",
    "text": "Sub-lesson 3: The good way to create synthetic data\n\nThink of dimensions of variability of user queries: An approach that I have learnt from Hamel Husain & Shreya Shankar is to first think about the different dimensions of variability in user queries for your specific application. For example, if youâ€™re building a RAG over a technical product documentation, you can think about several dimensions of variability, such as:\nUser type (user, admin, developer, etc.)\nIntent (seeking information, troubleshooting, feature requests, etc.)\nUser expertise level (beginner, intermediate, expert, etc.)\nQuery length (short queries, long queries, etc.)\nQuery complexity (simple queries, complex queries with multiple sub-questions, etc.)\nQuery style (formal, informal, typos, etc.)\netc.\n\nAs a query always depends on the context of the application and the persona of the users (a wink ğŸ˜‰ to the pre-lesson above), a dimension should be really specific to your application and not some general dimensions that someone else has used in another context.\nThen, for each dimension, you can brainstorm different values that the dimension can take (or delegate the task of brainstorming values of some dimensions to an LLM). For example, for the â€œuser expertise levelâ€ dimension, you can have the values: â€œbeginnerâ€, â€œintermediateâ€, â€œexpertâ€ as shown above.\nOnce the list of dimensions and their possible values is ready, you can start combining them to create tuples that will represent different synthetic queries.\nHere are some examples of tuples representing combinations of the dimensions mentioned above.\n(\"end_user\", \"troubleshoot\", \"beginner\", \"short\", \"simple\", \"typos\")\n(\"developer\", \"integration_info\", \"expert\", \"long\", \"complex\", \"formal\")\n(\"admin\", \"permissions_help\", \"intermediate\", \"short\", \"simple\", \"informal\")\n(\"end_user\", \"feature_discovery\", \"beginner\", \"short\", \"simple\", \"incomplete\")\n(\"support_engineer\", \"root_cause_analysis\", \"expert\", \"long\", \"complex\", \"dense\")\n(\"end_user\", \"account_status\", \"beginner\", \"short\", \"simple\", \"mixed_case\")\n(\"developer\", \"performance_optimization\", \"expert\", \"medium\", \"complex\", \"typos\")\n(\"admin\", \"audit_logging\", \"intermediate\", \"medium\", \"moderate\", \"formal\")\n(\"end_user\", \"error_meaning\", \"beginner\", \"short\", \"simple\", \"abbreviations\")\nEach tuple becomes a prompt seed you can use to generate multiple queries from ğŸš€\nAnd here you have it, asystematic way to create synthetic data that covers a wide range of possible user queries for your specific application"
  },
  {
    "objectID": "posts/LLM evaluation/LLM evaluation.html#sub-lesson-1-the-three-steps-for-error-analysis",
    "href": "posts/LLM evaluation/LLM evaluation.html#sub-lesson-1-the-three-steps-for-error-analysis",
    "title": "Stop Vibe-Checking: Real-World Lessons on LLM Evals",
    "section": "Sub-lesson 1: The three steps for error analysis",
    "text": "Sub-lesson 1: The three steps for error analysis\nIn short, the steps are: - Generate your systemâ€™s anwsers for the queries that you have (real or synthetic). Gather the traces of the execution for each query. - Open-coding: Note down the first failure that appears in the trace for each query (if present) - Axial coding: Cluster the failures into families of failure modes.\nThis three steps are the key to a successful error analysis. Since Iâ€™ve discovered this approach in Hamel & Shreyaâ€™s course, it has become my go-to approach for identifying failure modes in LLM applications in a systematic way.\n\nTraces are king\nA trace is a detailed log of the execution of your LLM application for a query. It includes all the intermediate steps, LLM calls, tool calls, and final output.\nWell, now that you have a set of user queries (either real or synthetic), input each query through your LLM application and generate the whole trace of the execution. The trace should include all the intermediate steps, LLM calls, tool calls, and final output.\nYou should go through a good number of queries to cover the diversity of your queries (usually around a 100 but really depends on the complexity of your application and the diversity of your user queries, could be more or could be less than a 100).\n\n\nOpen-coding\nOnce you have the traces, go through each trace one by one and note down the first failure that appears in the trace for each query (if present). The description of the failure should be a bit specific. For example, you can say â€œDidnâ€™t call the call_customer_tool even though the customer asked the system to do soâ€.\n\n\nAxial coding\nNow that you have annoatated the failures in your dataset, the next step is to put them into clusters of failure modes. You can do: - this manually by going through the list of failures and grouping them into families of failure modes - using an LLM to help you with the clustering. You can provide the LLM with the list of failures and ask it to group them into families of failure modes. This is usually my starting point as it saves a lot of time. Then, I go through the clusters and refine them if needed.\nNow, hereâ€™s a diagram that summarizes this 3-step process:\n\n\n\n\n\n\n\nErrorAnalysis\n\n\n\nStart\n\nStart:\nCollect Queries\n(Real or Synthetic)\n\n\n\nStep1\n\n1. Generate Traces\nRun queries through system\nCapture execution steps\n\n\n\nStart-&gt;Step1\n\n\n\n\n\nStep2\n\n2. Open-Coding\nAnnotate first failure\nin each trace\n\n\n\nStep1-&gt;Step2\n\n\n\n\n\nStep3\n\n3. Axial Coding\nCluster failures into\nfailure mode families\n\n\n\nStep2-&gt;Step3\n\n\n\n\n\nStep3-&gt;Start\n\n\nIterate\n\n\n\n\n\n\n\n\n\n\nWhat comes next ?\nPrioritization: Now having these clusters of failure modes will help you identify which exact parts of your system are responsible for failures and will help quantify how many failures are due to each failure mode.\nIteration: Once you have identified the failure modes and prioritized them, you can start working on fixing them. After fixing them, you can go through the same process again to identify new failure modes that are still there. Itâ€™s very important to know that error analysis is an iterative process. You need to do it two or three times at first.\n\n\nScale your evals in production\nYou might ask yourself if youâ€™ll be doing this manually over production data to verify if th? The answer is that once you have a good understanding of the failure modes and have fixed the most critical ones, you can scale your evaluation by running a custom LLM-as-a-judge for large samples of queries that you have in prod.\nHereâ€™s how to do it: - Get the traces that youâ€™ve labeled previously with successful and failed queries & split them into a train, a dev and a test set.\n\nCreate a custom LLM-as-a-judge for each failure mode:\n\nFor each failure mode, create a prompt that describes the failure mode in a very specific way. Donâ€™t be broad in your description and donâ€™t use vague word (like good, etc). and provides examples of successful and failed queries from your labeled dataset.\nProvide the LLM with example of both successful and failed queries from your train dataset to help it understand the failure mode better.\nEval the performance of the LLM-as-a-judge on your dev set and iterate on the prompt until you reach a satisfactory performance\nOnce youâ€™re satisfied with the performance on the dev set, test the LLM-as-a-judge on your test set to get an estimate of its performance on unseen data. This should give you a good idea of how well the LLM-as-a-judge will perform on production data.\n\n\nNow, this LLM-as-a-judge can be used to evaluate large samples of production queries and identify the failure modes that are still present in your system."
  },
  {
    "objectID": "posts/LLM evaluation/LLM evaluation.html#sub-lesson-2-clustering-production-queries-didnt-work-for-me",
    "href": "posts/LLM evaluation/LLM evaluation.html#sub-lesson-2-clustering-production-queries-didnt-work-for-me",
    "title": "Stop Vibe-Checking: Real-World Lessons on LLM Evals",
    "section": "Sub-lesson 2: Clustering production queries didnâ€™t work for me",
    "text": "Sub-lesson 2: Clustering production queries didnâ€™t work for me\nOne approach that Iâ€™ve tested in the past (amounts to 1 year) and that didnâ€™t work that well for me is to identify a set of classes that I expect user queries in my app to fall into. Identifying the classes helps in knowing which queries the model is not handling well and which classes of queries need more attention.\nOn top of the identified classes, I would create a category for â€œotherâ€ queries that donâ€™t fall into any of the identified classes. an analysis of the queries that have fallen into the â€œotherâ€ category helped me identify new classes that I hadnâ€™t thought about initially and better pinpoint the analysis of the performance of the model on different types of queries.\nAt the time, I used GPT-4 to do the classification. What didnâ€™t work well for me was most queries ended up in the â€œotherâ€ category, defeating the purpose of the classification. This pattern showed up with a couple of other decoder LLMs.\nWhile I thought about finetuning a Bert-like model to do the classification, having to finetune a new model whenever a new class is identified seemed like an overkill and a maintenance nightmare.\n\n\n\n\n\n\n\nG\n\n\n\nStart\n\nDefine Query Classes\n+ 'Other' Category\n\n\n\nClassify\n\nGPT-4\nClassification\n\n\n\nStart-&gt;Classify\n\n\n\n\n\nToClass\n\nFew Queries:\nDefined Classes\n\n\n\nClassify-&gt;ToClass\n\n\nFew\n\n\n\nToOther\n\nMost Queries:\n'Other' Category\n\n\n\nClassify-&gt;ToOther\n\n\nMost\n\n\n\nProblem\n\nâš ï¸ PROBLEM:\nMost queries in 'Other'\nDefeats Purpose!\n\n\n\nToClass-&gt;Problem\n\n\n\n\n\nToOther-&gt;Problem\n\n\n\n\n\nConsiderBERT\n\nTry BERT\nFinetuning?\n\n\n\nProblem-&gt;ConsiderBERT\n\n\n\n\n\nProblem2\n\nâš ï¸ PROBLEM:\nMust retrain for\neach new class\nMaintenance Nightmare!\n\n\n\nConsiderBERT-&gt;Problem2"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Lead Data Scientist | Paris, France\nchsafouane@gmail.com | LinkedIn | GitHub\n\n\n\nML/AI Domains: NLP/LLMs, RAG Systems, Search & Recommendation Engines, Computer Vision Cloud & Infrastructure: GCP (Vertex AI, BigQuery), AWS, Docker, Kubernetes, MLflow, Airflow Specialized: LLM Evaluation, Production ML Monitoring, Model Drift Detection, Agentic Systems\n\n\n\n\n\n\n\nBuilt production agentic system for insurance companies using Google ADK framework and Vertex AI Gemini API\nDesigned multi-modal pipeline integrating ElevenLabs voice synthesis with Gemini-powered insurer case analysis\nArchitected end-to-end solution on GCP: Cloud Run for serving, BigQuery for analytics, Cloud Storage for document ingestion\n\n\n\n\n\nArchitected document parsing and RAG pipeline on GCP using Vertex AI embeddings and Vector Search for semantic retrieval over internal knowledge bases\nBuilt LLM evaluation framework leveraging Vertex AI Model Evaluation to benchmark Gemini and custom models on business-critical tasks\nDeployed production inference endpoints using Vertex AI Prediction with automated scaling and monitoring\n\n\n\n\n\nArchitected and deployed a RAG system reducing customer support costs by 20%\nEnhanced phone assistant system that allows patients to make a medical appointment using a fast search system and agents\n\n\n\n\n\nBuilt & deployed an LLM-powered catalog integration system reducing products onboarding time from 20+ days to hours\nImplemented end-to-end MLOps pipeline for model versioning and automated retraining using MLflow\nTrained & Deployed small LLMs for domain-specific tasks to reduce costs of third-party API usage by 10X\n\n\n\n\n\nDesigned recommendation algorithms driving 35% increase in page visits across web, mobile, and email channels\nPioneered LLM-based review summarization system processing 500K+ reviews monthly\nOptimized search functionality using semantic embeddings (Elasticsearch, pgvector)\nBuilt collaborative filtering and content-based recommendation models serving 20M+ users\n\n\n\n\n\nDeployed production ML models for customer segmentation, churn prediction (AUC 0.92), and LTV modeling\nDesigned real-time drift detection system monitoring 15+ models in production environment\nBuilt A/B testing framework and interactive dashboard reducing experiment analysis time by 60%\nDeveloped predictive maintenance models using signal processing and deep learning (CNN/RNN architectures)\n\n\n\n\n\n\n\nProduction Impact: Deployed 20+ ML models to production including Vertex AI endpoints with measurable business and costs impact\nTechnical Leadership: Mentored 5+ junior data scientists, conducted Python/R training sessions at EDF\nCross-functional Collaboration: Led projects spanning engineering, product, and business teams\n\n\n\n\n\n\nMSc Engineering - INSA Lyon (2013-2018)\nDeep Learning Specialization - Coursera\nStatistical Learning - Stanford Online\n\n\n\n\n\n\nEnglish (Native)\nFrench (Professional)\nArabic (Native)\nSpanish (Fluent)\nMandarin (In the learning process)"
  },
  {
    "objectID": "resume.html#technical-expertise",
    "href": "resume.html#technical-expertise",
    "title": "Resume",
    "section": "",
    "text": "ML/AI Domains: NLP/LLMs, RAG Systems, Search & Recommendation Engines, Computer Vision Cloud & Infrastructure: GCP (Vertex AI, BigQuery), AWS, Docker, Kubernetes, MLflow, Airflow Specialized: LLM Evaluation, Production ML Monitoring, Model Drift Detection, Agentic Systems"
  },
  {
    "objectID": "resume.html#professional-experience",
    "href": "resume.html#professional-experience",
    "title": "Resume",
    "section": "",
    "text": "Built production agentic system for insurance companies using Google ADK framework and Vertex AI Gemini API\nDesigned multi-modal pipeline integrating ElevenLabs voice synthesis with Gemini-powered insurer case analysis\nArchitected end-to-end solution on GCP: Cloud Run for serving, BigQuery for analytics, Cloud Storage for document ingestion\n\n\n\n\n\nArchitected document parsing and RAG pipeline on GCP using Vertex AI embeddings and Vector Search for semantic retrieval over internal knowledge bases\nBuilt LLM evaluation framework leveraging Vertex AI Model Evaluation to benchmark Gemini and custom models on business-critical tasks\nDeployed production inference endpoints using Vertex AI Prediction with automated scaling and monitoring\n\n\n\n\n\nArchitected and deployed a RAG system reducing customer support costs by 20%\nEnhanced phone assistant system that allows patients to make a medical appointment using a fast search system and agents\n\n\n\n\n\nBuilt & deployed an LLM-powered catalog integration system reducing products onboarding time from 20+ days to hours\nImplemented end-to-end MLOps pipeline for model versioning and automated retraining using MLflow\nTrained & Deployed small LLMs for domain-specific tasks to reduce costs of third-party API usage by 10X\n\n\n\n\n\nDesigned recommendation algorithms driving 35% increase in page visits across web, mobile, and email channels\nPioneered LLM-based review summarization system processing 500K+ reviews monthly\nOptimized search functionality using semantic embeddings (Elasticsearch, pgvector)\nBuilt collaborative filtering and content-based recommendation models serving 20M+ users\n\n\n\n\n\nDeployed production ML models for customer segmentation, churn prediction (AUC 0.92), and LTV modeling\nDesigned real-time drift detection system monitoring 15+ models in production environment\nBuilt A/B testing framework and interactive dashboard reducing experiment analysis time by 60%\nDeveloped predictive maintenance models using signal processing and deep learning (CNN/RNN architectures)"
  },
  {
    "objectID": "resume.html#key-achievements-leadership",
    "href": "resume.html#key-achievements-leadership",
    "title": "Resume",
    "section": "",
    "text": "Production Impact: Deployed 20+ ML models to production including Vertex AI endpoints with measurable business and costs impact\nTechnical Leadership: Mentored 5+ junior data scientists, conducted Python/R training sessions at EDF\nCross-functional Collaboration: Led projects spanning engineering, product, and business teams"
  },
  {
    "objectID": "resume.html#education-certifications",
    "href": "resume.html#education-certifications",
    "title": "Resume",
    "section": "",
    "text": "MSc Engineering - INSA Lyon (2013-2018)\nDeep Learning Specialization - Coursera\nStatistical Learning - Stanford Online"
  },
  {
    "objectID": "resume.html#languages",
    "href": "resume.html#languages",
    "title": "Resume",
    "section": "",
    "text": "English (Native)\nFrench (Professional)\nArabic (Native)\nSpanish (Fluent)\nMandarin (In the learning process)"
  },
  {
    "objectID": "posts/Pixi intro/Why I ditched conda and pip for Pixi.html",
    "href": "posts/Pixi intro/Why I ditched conda and pip for Pixi.html",
    "title": "Why I ditched pip and conda for Pixi",
    "section": "",
    "text": "Iâ€™ve been using pip and then conda for as long as I can remember. Last year, I did a double-switch. First, I moved to uv and then not long after it I moved to pixi.\nI discovered pixi thanks to Eric Ma blog post. At the time, the thing that caught my attention the most is how easy it is to manage the installation of the same environment but one with CUDA support and the other without.\nAfter months of using pixi now, I can say that the 3 things I like the most about pixi are: - The features concept that allows to mix and match packages to create environments - Being able to run tasks - How fast it is!\nWeâ€™ll take a look at all of this in this blog. The final version of the code generated in this blog is available in this repository."
  },
  {
    "objectID": "posts/Pixi intro/Why I ditched conda and pip for Pixi.html#pixis-toolset",
    "href": "posts/Pixi intro/Why I ditched conda and pip for Pixi.html#pixis-toolset",
    "title": "Why I ditched pip and conda for Pixi",
    "section": "Pixiâ€™s toolset",
    "text": "Pixiâ€™s toolset\nNow, Pixi is many things but Iâ€™ll focus on the things that will be of use to you as a python developer\n\nPixi is a package manager that can manage packages from both Conda & PyPI. The dependency resolution tools used by Pixi (resolvo for conda & uv resolution tool for PyPI packages) are very fast.\nPixi manages environments (similar to venv for pip users, a feature that is built into conda)\nPixi manages python version as well (similar to pyenv if you use pip, built into conda)\nPixi has a lock file that allows you to reproduce excatly the same environment (similar to what youâ€™d get conda-lock or pip-lock)\nPixi can be used as a task-runner, just like make or just.\nPixi has built-in cross-platform reproducibility. The lock file includes the exact versions and dependencies in all targeted platforms. You can pick and choose the targeted platform by your project (Linux, Windows, etc).\nPixi can also install tools like brew and you can have access to the globally.\n\nNow, while mamba is fast, in my experience, pixi is faster. mamba also lacks lock-files that are essential for reproducibility and a task runner that comes very handy in many situations (CI/CD, Other people running your project, etc.)"
  },
  {
    "objectID": "posts/Pixi intro/Why I ditched conda and pip for Pixi.html#pixis-project-philosophy",
    "href": "posts/Pixi intro/Why I ditched conda and pip for Pixi.html#pixis-project-philosophy",
    "title": "Why I ditched pip and conda for Pixi",
    "section": "Pixiâ€™s project philosophy",
    "text": "Pixiâ€™s project philosophy\nWhile conda is environment-centric, pixi is all about projects. When you init a pixi project, it will create a pixi.toml (or a pyproject.toml instead if you want). In this file, you can specify many environments that can be composed of different features. For example, you can have: - a base feature that includes the basic packages needed by your project - a run feature that consists of packages needed for only running the project - a test feature that consists of additional packages needed for testing the project - a build feature that consists of additional packages or tools needed for building the project.\nImagine having to train a model on a GPU but then when running it, to only have a CPU at your disposal. What you would do is have: - A training environment composed of the features base + build + test that will include the base packages, some CUDA dependencies and pytorch with GPU support coming from the build feature, and test packages like pytest coming from the test feature. - A CI/CD environment composed of base + run + test. The only difference this time is that youâ€™ll be using the run feature that include pytorch-cpu and no CUDA dependencies. - A run environment composed only of base + run features.\nThe other nice thing is that you can say that you can enforce that some environments (or all of them) use the same versions of the common packages."
  },
  {
    "objectID": "posts/Pixi intro/Why I ditched conda and pip for Pixi.html#installation",
    "href": "posts/Pixi intro/Why I ditched conda and pip for Pixi.html#installation",
    "title": "Why I ditched pip and conda for Pixi",
    "section": "Installation",
    "text": "Installation\nStart first by installing pixi by grabbing the one command-line that corresponds to your case from here: Pixi installation.\nItâ€™s really just one command, restart your terminal and there you go."
  },
  {
    "objectID": "posts/Pixi intro/Why I ditched conda and pip for Pixi.html#getting-hands-on",
    "href": "posts/Pixi intro/Why I ditched conda and pip for Pixi.html#getting-hands-on",
    "title": "Why I ditched pip and conda for Pixi",
    "section": "Getting hands-on",
    "text": "Getting hands-on\n\nInitiating the project\nWeâ€™ll work through an example where weâ€™d like to develop a FastAPI app.\nLet us initiate a pixi project:\npixi init fastapi_app --format pyproject\ncd fastapi_app\nIf you already have an existing folder, you can simply go inside of it and execute\npixi init --format pyproject\nBy default, pixi uses a pixi.toml file for its configuration. As people in python use pyproject.toml, you can specify that you want to use the latter with the --format pyproject.\nThe initiation of the project creates the following files:\n\n\n\nIf you look at the content of the pyproject.toml, youâ€™ll see different sections:\n[project]\nauthors = [{name = \"Safouane Chergui\", email = \"chsafouane@gmail.com\"}]\nname = \"fastapi_app\"\nrequires-python = \"&gt;= 3.11\"\nversion = \"0.1.0\"\ndependencies = [ \"fastapi&gt;=0.115.14,&lt;0.116\", \"uvicorn[standard]&gt;=0.35.0,&lt;0.36\"]\n\n[build-system]\nbuild-backend = \"hatchling.build\"\nrequires = [\"hatchling\"]\n\n[tool.pixi.workspace]\nchannels = [\"conda-forge\"]\nplatforms = [\"win-64\"]\n\n[tool.pixi.pypi-dependencies]\nfastapi_app = { path = \".\", editable = true }\n\n[tool.pixi.tasks]\nLet us dive into the most important fields:\nThe [project] section includes project metadata. - As we havenâ€™t added a specific python interpreter to the project, the requires-python entry shows the currently active python interpreter in the terminal. You can change it manually if you want.\nThe [tool.pixi.workspace] section has two entries: - The channels shows the conda channels that can be used to download the conda packages.If you have a company repository (like nexus), it can be used instead or added before conda-forge to be used first. - The platforms corresponds to the platform youâ€™re using. You can add other platforms here and the pixi.lock will include the packages that need to be installed to reproduce the exact environment in the case of the additional platforms.\nThe [tool.pixi.pypi-dependencies] section is used to specify the packages to install from PyPI. By default, the code youâ€™re developping shows up as an editable package. Your code will be installed in editable mode and youâ€™ll be able to see the changes you make to your code directly reflected in your environment.\nThe [tool.pixi.tasks] section is empty for the time-being. You can imagine tasks as a replacement of makefiles. Weâ€™ll add some tasks later in the blog post.\n\n\nAdding dependencies\nLet us add python 3.12 to the project\npixi add python=3.12\nAs weâ€™re going to create a FastAPI app, let us add fastapi and uvicorn but this time from PyPI.\npixi add --pypi fastapi \"uvicorn[standard]\"\nNow that we have proceeded with adding these dependencies, we can see that we have a pixi.lock file that was created.\n\n\n\nThe pyproject.toml file is now updated to include the new dependencies:\n[project]\nauthors = [{name = \"Safouane Chergui\", email = \"chsafouane@gmail.com\"}]\nname = \"fastapi_app\"\nrequires-python = \"&gt;= 3.11\"\nversion = \"0.1.0\"\ndependencies = [ \"fastapi&gt;=0.115.14,&lt;0.116\", \"uvicorn[standard]&gt;=0.35.0,&lt;0.36\"]\n\n[build-system]\nbuild-backend = \"hatchling.build\"\nrequires = [\"hatchling\"]\n\n[tool.pixi.workspace]\nchannels = [\"conda-forge\"]\nplatforms = [\"win-64\"]\n\n[tool.pixi.pypi-dependencies]\nfastapi_app = { path = \".\", editable = true }\n\n[tool.pixi.tasks]\n\n[tool.pixi.dependencies]\npython = \"3.12.*\"\n\nPinning strategy\nThe thing that bothered me the most when I started with pixi is that the pinning of the packages. By default, pixi will use a very strict pinning strategy as you can see with fastapi for example: \"fastapi&gt;=0.115.14,&lt;0.116\", even if the user didnâ€™t specify a version when adding fastapi.\nIf later youâ€™d like to install a package that is not compatible with the pinned version of fastapi (even though you donâ€™t care about the specific minor version of fastapi shown in the pyproject.toml, or the upper bound constraint), youâ€™ll get an error, and this was frustrating.\npixi developers explain why they chose this strategy and discuss the matter at length in this GitHub issue.\nNonetheless, you can override the pinning strategy by using the pinning-strategy configuration but weâ€™ll look at pixiâ€™s config file later.\n\n\n\nManaging environments with features\nOne of pixiâ€™s amazing features is being able to manage different sets of dependencies for different purposes (like the example for the run, build, test, etc above) using features. A feature (also called a dependency group is just a named set of dependencies).\nBy default, when adding packages, pixi will automatically add packages to the standard group of dependencies. You can add packages to a specific feature by using the --feature flag.\nLetâ€™s say that our core dependencies that are needed for running the app are fastapi and uvicorn. Let us add two families of dependencies (two features): - A test feature that will include pytest & pytest-cov\npixi add --feature test pytest pytest-cov\n\nA dev feature that will include packages needed for development like ruff\n\npixi add --feature dev ruff\nWhen youâ€™ll add this second feature, youâ€™ll get a warning saying that the test feature was added but is not used by any environment and that is ok as weâ€™re going to do it just after.\nNow, if you look at the pyproject.toml file, youâ€™ll see that the dependencies are now grouped by features:\n[project]\nauthors = [{name = \"Safouane Chergui\", email = \"chsafouane@gmail.com\"}]\nname = \"fastapi_app\"\nrequires-python = \"&gt;= 3.11\"\nversion = \"0.1.0\"\ndependencies = [ \"fastapi&gt;=0.115.14,&lt;0.116\", \"uvicorn[standard]&gt;=0.35.0,&lt;0.36\"]\n\n[build-system]\nbuild-backend = \"hatchling.build\"\nrequires = [\"hatchling\"]\n\n[tool.pixi.workspace]\nchannels = [\"conda-forge\"]\nplatforms = [\"win-64\"]\n\n[tool.pixi.pypi-dependencies]\nfastapi_app = { path = \".\", editable = true }\n\n[tool.pixi.tasks]\n\n[tool.pixi.dependencies]\npython = \"3.12.*\"\n\n[tool.pixi.feature.test.dependencies]\npytest = \"*\"\npytest-cov = \"*\"\n\n[tool.pixi.feature.dev.dependencies]\nruff = \"*\"\n\nCreating environments from dependency groups (features)\nIn pixi, every environment is a collection of features (can be two features or more). The main project dependencies added without any feature like fastapi and uvicorn are added to an implicit default feature and to a default environment. If you execute\n pixi project environment list\nYouâ€™ll see that the default environment is called default and it includes the default feature.\nEnvironments:\n- default:\n    features: default\nWhen you create a feature like test, pixi will create an environment from the default feature + the test feature, unless you explicitly say that you donâ€™t want to do so. This means, that by default, the test environment isnâ€™t composed of just the dependencies in the test feature but also the dependencies in the default feature: - All dependencies from the default feature (fastapi, uvicorn) - All dependencies from the test feature (pytest, pytest-cov)\nBefore creating the environments, let us tackle one last thing: the solve-groups.\nImagine having the default environment that includes fastapi and uvicorn and a test environment that includes additionally pytest and pytest-cov. When pixi will resolve the dependencies, the default environment can have different versions of fastapi and uvicorn than the test environment. To force pixi to group both environments together at the solve stage, you need to say that the test environment should be solved together with the default environment by using the --solve-groups flag.\nHereâ€™s the documentation definition of the --solve-groups flag:\n\nsolve-group: String: The solve group is used to group environments together at the solve stage. This is useful for environments that need to have the same dependencies but might extend them with additional dependencies. For instance when testing a production environment with additional test dependencies.\n\nLet us create the environments now:\nTest environment:\nWeâ€™re saying that we want to create a test_env environment that includes the test feature and that we want to solve it together with the default environment (the one that includes fastapi and uvicorn).\npixi project environment add fastapi-test-env --feature test --solve-group default\nDev environment:\nWeâ€™re saying that we want to create a test_env environment that includes the test feature and that we want to solve it together with the default environment (the one that includes fastapi and uvicorn).\npixi project environment add fastapi-dev-env --feature test --feature dev --solve-group default\nNow, if you list the environments, youâ€™ll see that the test_env and dev_env are created and that they include the features we specified:\npixi project environment list\n\n\nEnvironments:\n- default:\n    features: default\n- fastapi-test-env:\n    features: test, default\n    solve_group: default\n- fastapi-dev-env:\n    features: test, dev, default\n    solve_group: default\nIf you look at the pyproject.toml file, youâ€™ll see that you have a new section called [tool.pixi.environments] that includes the environments you created:\n[tool.pixi.environments]\nfastapi-test-env = { features = [\"test\"], solve-group = \"default\" }\nfastapi-dev-env = { features = [\"test\", \"dev\"], solve-group = \"default\" }\nAll of this can be added manually instead to the pyproject.toml but itâ€™s error prone and the pixi CLI is honestly very handy.\n\n\nEnvironments installation\nNow, let us create the environments by first install the default environment\npixi install\nYou can also simply run pixi shell to install the default environment and open a shell in it.\nTo install the dev environment, you can run:\npixi install fastapi-dev-env\nYou can also install all the environments at once using the flag --all to install:\npixi install --all\nNow you can any one of the environment inside the shell by running for example:\npixi shell fastapi-dev-env"
  },
  {
    "objectID": "posts/Pixi intro/Why I ditched conda and pip for Pixi.html#creating-tasks",
    "href": "posts/Pixi intro/Why I ditched conda and pip for Pixi.html#creating-tasks",
    "title": "Why I ditched pip and conda for Pixi",
    "section": "Creating tasks",
    "text": "Creating tasks\nTo create a task, you can use the pixi task add command and youâ€™ll have to specify two things:\n\nThe task name\nThe command to run\n\nExecute pixi task add --help to see the available options, as you can add for example environment variables or isolate the task from the shell when running (not having access to the shell variables for example) among other things.\nLet us create a task to start a uvicorn server with hot reloading. The task will have as a name start. The command will add the task to the pyproject.toml file under the [tool.pixi.tasks] section.\npixi task add start \"uvicorn my_app.main:app --reload --host 0.0.0.0\"\nLet us also add a linting task that uses ruff\npixi task add lint \"ruff check src --fix\"\nIf you look now at the pyproject.toml file, youâ€™ll see that the tasks are added under the [tool.pixi.tasks] section:\n[tool.pixi.tasks]\nstart = \"uvicorn my_app.main:app --reload --host 0.0.0.0\"\nlint = { task = \"ruff check src --fix\", environment = \"fastapi-dev-env\" }\n\nRunning tasks\nTo run the linting task in the dev environment, you can run:\npixi run -e fastapi-dev-env lint\nYouâ€™ll get the following output:\nPixi task (lint in fastapi-dev-env): ruff check src --fix\nAll checks passed!\nNow, you can specify in the pyproject.toml the default environment in which the task should run but I havenâ€™t found a way to do it through the CLI yet.\n[tool.pixi.tasks]\nstart = \"uvicorn my_app.main:app --reload --host 0.0.0.0\"\nlint = { task = \"ruff check src --fix\", environment = \"fastapi-dev-env\" }\nAs I canâ€™t go through everything you can do with tasks, Iâ€™ll just list the things that I find useful but you can find more in the pixi documentation:\n\nYou can create a task that is composed of many tasks using the dependes-on field. that for example executes the linting task and then runs the app\nYou can create a tasks that runs the same task in multiple environments. If for example youâ€™d like to test your code against multiple python versions, you can create a task that runs the same task in environments with different python versions (instead of using matrices of environments in CI/CD). Here an example from pixiâ€™s documentation:\n\n# Task that depends on other tasks in different environments\n[tasks.test-all]\ndepends-on = [\n  { task = \"test\", environment = \"py311\" },\n  { task = \"test\", environment = \"py312\" },\n]\n\nYou can add environment variables or isolate the task when running from the shell (and thus not having access to the shell variables).\nIf a task depends on another task, you can cache the result of the first task and use it in the second task. Pixi wonâ€™t rerun the first task after doing some verifications that can be found in the documentation."
  },
  {
    "objectID": "posts/Pixi intro/Why I ditched conda and pip for Pixi.html#why-another-config-file",
    "href": "posts/Pixi intro/Why I ditched conda and pip for Pixi.html#why-another-config-file",
    "title": "Why I ditched pip and conda for Pixi",
    "section": "Why another config file ?",
    "text": "Why another config file ?\nThe pyproject.toml (or the pixi.toml) file reprensents the configuration of the pixi project. It includes the project metadata, the dependencies, the environments, the tasks, etc.\nThere is additional configuration that is not required for the project per say but in a way changes the behavior you would place in a config.toml file.\nYou can set this config at one of three levels: - locally: in this case, the configuration will be stored your_project/.pixi/config.toml and will impact only the current project. - globally: in this case, the configuration will be stored in $PIXI_HOME/config.toml and will impact all the projects using pixi. - system-wide: in this case, the configuration will be stored in /etc/pixi/config.toml and will impact all the projects using pixi.\nYou can also use the pixi config set &lt;some_config_key&gt; &lt;some_config_value&gt; command to set the configuration. While I will show you right away the keys that I find useful, you can find the full list of configuration keys as of version 0.49 that you can set:\n  â”‚ Supported keys:\n  â”‚     default-channels,\n  â”‚     authentication-override-file,\n  â”‚     tls-no-verify,\n  â”‚     mirrors,\n  â”‚     detached-environments,\n  â”‚     pinning-strategy,\n  â”‚     max-concurrent-solves,\n  â”‚     repodata-config,\n  â”‚     repodata-config.disable-jlap,\n  â”‚     repodata-config.disable-bzip2,\n  â”‚     repodata-config.disable-zstd,\n  â”‚     repodata-config.disable-sharded,\n  â”‚     pypi-config,\n  â”‚     pypi-config.index-url,\n  â”‚     pypi-config.extra-index-urls,\n  â”‚     pypi-config.keyring-provider,\n  â”‚     shell,\n  â”‚     shell.force-activate,\n  â”‚     shell.source-completion-scripts,\n  â”‚     shell.change-ps1,\n  â”‚     s3-options,\n  â”‚     s3-options.&lt;bucket&gt;,\n  â”‚     s3-options.&lt;bucket&gt;.endpoint-url,\n  â”‚     s3-options.&lt;bucket&gt;.region,\n  â”‚     s3-options.&lt;bucket&gt;.force-path-style,\n  â”‚     experimental.use-environment-activation-cache,\n  â”‚     proxy-config,\n  â”‚     proxy-config.https,\n  â”‚     proxy-config.http,\n  â”‚     proxy-config.non-proxy-hosts"
  },
  {
    "objectID": "posts/Pixi intro/Why I ditched conda and pip for Pixi.html#useful-keys",
    "href": "posts/Pixi intro/Why I ditched conda and pip for Pixi.html#useful-keys",
    "title": "Why I ditched pip and conda for Pixi",
    "section": "Useful keys",
    "text": "Useful keys\n\nUsing private conda & PyPI repositories\nSome of my very security-oriented customers usually have their own conda and pip repositories (like nexus) and oblige everyone to use them as they only include packages that are approved by the security team.\nFor this, I use pypi-config.index-url and pypi-config.extra-index-urls to specify the index URL and the extra index URLs to use for PyPI packages.\npixi config set pypi-config.index-url https://nexus.some_random_company.com/pypi/simple\nLooking at the documentation, these can also be added to the pyproject.toml file under the [tool.pixi.pypi-options] section but Iâ€™ve never added them here.\n[tool.pixi.pypi-options]\n# Public packages will be sourced from the official PyPI\nindex-url = \"https://nexus.some_random_company.com/pypi/simple\"\n# Internal packages will be searched for here first\nextra-index-urls = [\"https://nexus.some_additional_random_company.com/pypi/simple\"]\nFor conda, I add the channels to the channels entry under the [tool.pixi.workspace] section in the pyproject.toml file:\n[tool.pixi.workspace]\nchannels = [\n    \"https://nexus.some_random_company.com/conda-forge\", \n    \"https://nexus.some_random_company_second.com/conda-forge\"\n]\nplatforms = [\"win-64\"]\nIf you need to manage credentials for private repositories, you can check pixi auth login.\n\n\nPinning strategy\nThe other key that I find useful is the pinning-strategy key. As I said before, by default, pixi uses a very strict pinning strategy that can be annoying at times. You can change it to one of the strategies listed in the documentation.\nPersonally, I like to pin to the major version using:\npixi config set pinning-strategy major\nThis might not be a very good practice as you can see here but it works just fine for my needs."
  },
  {
    "objectID": "posts/SwiGLU/All you need to know about SwiGLU.html",
    "href": "posts/SwiGLU/All you need to know about SwiGLU.html",
    "title": "Why does SwiGLU work ?",
    "section": "",
    "text": "The goal of this blog post is to explain why modern LLM architectures use SwiGLU as the activation function for the feed-forward part and have moved away from ReLU.\nTable of contents\n- Q1: Why do we need activation functions at all?\n- Q2: Whatâ€™s wrong with ReLU?\n- Q3: What is the Swish activation function?\n- Q4: What are Gated Linear Units (GLU)?\n- Q5: What is SwiGLU then?\n- Final note\n\n\n\nQ1: Why do we need activation functions at all?\nConsider this: a neural network is essentially a series of matrix multiplications. If we stack linear layers without any activation function:\n\\[y = W_3(W_2(W_1 x))\\]\nThis simplifies to:\n\\[y = (W_3 W_2 W_1) x = W_{combined} x\\]\nNo matter how many layers you stack, itâ€™s still just a linear transformation. The network can only learn linear relationships.\nActivation functions introduce non-linearity, allowing the network to approximate complex, non-linear functions. This is the foundation of deep learningâ€™s expressive power.\nNow, if you have a hard time mentally visualizing the impact of applying activation functions, I highly advise you to watch the section 3 of this video from Alfredo Canzianiâ€™s Deep Learning course. It will help you build a great intuition!\n\n\nQ2: Whatâ€™s wrong with ReLU?\nReLU literally revolutionized deep learning:\n\\[\\text{ReLU}(x) = \\max(0, x)\\]\nItâ€™s simple, fast, and solves the vanishing gradient problem that is a problem with functions like sigmoid or tanh.\nWhile people usually list problems that might be encountered when ReLU is used like the dying neuron problem, these problems are either theoritical or can be well managed most of the time with techniques used in neural networks nowadays (batch normalization, adaptive learning weights, etc)\n\n\nQ3: What is the Swish activation function?\nNow, before moving to SwiGLU, weâ€™ll look at an activation function Swish that is part of SwiGLU\n\\[\\text{Swish}(x) = x \\cdot \\sigma(x) = \\frac{x}{1 + e^{-x}}\\]\nSwish is a â€˜self-gatedâ€™ activation function: the input \\(x\\) is multiplied by its own sigmoid \\(\\sigma(x)\\), which acts as a gate that controls how much of the input passes through.\nLooking at how the gate behaves: - When \\(x\\) is very negative: \\(\\sigma(x) \\approx 0\\), so the gate is closed (suppresses the output) - When \\(x\\) is very positive: \\(\\sigma(x) \\approx 1\\), so the gate is fully open (passes the input through almost unchanged)\nDespite the bit more complicated formula, Swish has a very similar behavior to ReLU.\n\n\n\n\n\n\n\n\n\n\nIs Swish better than ReLU?\nEmpirically, Swish is found to work better than ReLU but like many things in deep learning, we donâ€™t know for sure why Swish works better, but here are the key differences:\n1. No hard gradient cutoff\nLooking at the plot above, the key difference is how they handle negative inputs:\n\nReLU: Hard cutoff at zero\n\nWhen \\(x &lt; 0\\): output = 0 & gradient = 0 (exactly)\nThis is the dying neuron problem (though as mentioned in Q2, itâ€™s often manageable with modern techniques like BatchNorm)\n\nSwish: Smooth, gradual approach to zero\n\nFor negative \\(x\\): gradient approaches zero asymptotically but never exactly hits zero for finite values\nNeurons can theoretically always receive updates (though updates may be negligible for very negative inputs)\n\n\n2. Smoothness\nReLU has a discontinuity at \\(x = 0\\) (derivative jumps from 0 to 1). Swish is infinitely differentiable everywhere, which means that the gradient landscape is smooth. Whether this smoothness contributes to the performance of Swish is not 100% clear but itâ€™s plausible that this helps with optimization\n\n\n\n\n\n\n\n\n\n\n\n\nQ4: What are Gated Linear Units (GLU)?\nNow, this is our last component before getting to SwiGLU. Letâ€™s talk about GLU.\n\\[\\text{GLU}(x, W, V, b, c) = (xW + b) \\odot \\sigma(xV + c)\\]\nWhere: - \\(x\\) is the input - \\(W, V\\) are weight matrices - \\(b, c\\) are bias vectors - \\(\\odot\\) is element-wise multiplication - \\(\\sigma\\) is the sigmoid function\nThe first thing that you should note is that GLU uses a gating mechanism and is somehow similar to Swish in that manner. The difference is that instead of applying the same transformation (identity) to all features and then gating with a fixed function (sigmoid), GLU uses two separate linear projections:\n\n\\(xW + b\\): this just takes the input & transforms it. It is usually called the content path\n\\(\\sigma(xV + c)\\): this second part says how much of the content of each feature should pass through and for that, itâ€™s called the gate path\n\nSo, GLU can really be thought of as a generalization fo Swish\n\nWhy is multiplicative gating powerful?\nThe element-wise multiplication \\(\\odot\\) allows the gate to select which element of the content to let pass through. The gate can completely suppress certain features (when \\(\\sigma(xV + c) \\approx 0\\)) while fully passing through others (when \\(\\sigma(xV + c) \\approx 1\\)).\n\n\nConcrete example of gating\nlet suppose we have a 4-dim vector \\(x\\)\n\\[x = [1.0, -0.5, 2.0, 0.3]\\]\nGLU applies 2 transformations to this same input:\n\nA transformation to the content through the content path: \\(xW + b\\). Let us say that it produces \\([2.0, -1.5, 3.0, 0.5]\\)\nA 2nd transformation thatâ€™s supposed to play the role of the gate: \\(\\sigma(xV + c)\\). Let us say that it produces \\([0.9, 0.1, 0.95, 0.05]\\)\n\nThe GLU output is their element-wise product:\n\\[\\text{GLU output} = [2.0 \\times 0.9, \\;\\; -1.5 \\times 0.1, \\;\\; 3.0 \\times 0.95, \\;\\; 0.5 \\times 0.05]\\] \\[= [1.8, \\;\\; -0.15, \\;\\; 2.85, \\;\\; 0.025]\\]\nThis means that: - Feature 1: Content is positive (2.0), gate is high (0.9) â†’ passes through strongly (1.8) - Feature 2: Content is negative (-1.5), gate is low (0.1) â†’ blocked (-0.15) - Feature 3: Content is positive (3.0), gate is very high (0.95) â†’ fully passes (2.85) - Feature 4: Content is small (0.5), gate is very low (0.05) â†’ suppressed (0.025)\nThis allows the network to learn complex decision rules: â€œfor inputs like \\(x\\), amplify feature 1 and 3, but suppress features 2 and 4.â€\n\n\n\n\n\n\n\n\n\n\n\n\nQ5: What is SwiGLU then?\nNow we have all the pieces. SwiGLU (Swish-Gated Linear Unit) simply combines Swish and GLU:\n\\[\\text{SwiGLU}(x, W, V) = \\text{Swish}(xW) \\odot xV\\]\nThatâ€™s it. Instead of using sigmoid for the gate (like in GLU), it uses Swish. Thatâ€™s why itâ€™s called Swish + GLU.\nSo what does each part of the formula do ? Well, itâ€™s exactly the same logic as GLU as what changes is just the gating function.\n\n\\(\\text{Swish}(xW)\\): The gate - decides how much of each feature passes through\n\\(xV\\): The content - the actual information being transmitted\n\\(\\odot\\): Element-wise multiplication - applies the gate to the content\n\n\n\n\n\n\n\n\n\n\n\nWhy does SwiGLU work so well?\nEmpirically, SwiGLU outperforms other activation functions in LLMs (even though not sure about VLMs for now). But why? Hereâ€™s the intuition:\n1. Multiplicative interactions create feature combinations\nThis is the key insight. Consider what each architecture computes:\nStandard FFN (ReLU/GELU): output = activation(xWâ‚) @ Wâ‚‚\nEach output dimension is a weighted sum of activated features. The activation is applied element-wise, this means that the features donâ€™t interact with each other inside the activation.\nSwiGLU FFN: output = (Swish(xW) âŠ™ xV) @ Wâ‚‚\nThe element-wise multiplication \\(\\odot\\) creates products between the two paths. If we denote \\(g = \\text{Swish}(xW)\\) and \\(c = xV\\), then output dimension \\(i\\) before the final projection is \\(g_i \\times c_i\\).\nHereâ€™s why this matters: both \\(g_i\\) and \\(c_i\\) are linear combinations of input features (before the Swish). Their product contains cross-terms like \\(x_j \\times x_k\\). The network can learn \\(W\\) and \\(V\\) such that certain input feature combinations are amplified or suppressed.\nThis is similar to why attention is powerful. Attention computes \\(\\text{softmax}(QK^T)V\\), where the \\(QK^T\\) product captures interactions between query and key features. SwiGLU brings a similar multiplicative expressiveness to the FFN.\n2. Why not use sigmoid in the gate instead of Swish?\nGLU uses sigmoid: \\(\\sigma(xW) \\odot xV\\). The problem with the sigmoid is that it saturate saturates. For large positive or negative inputs, \\(\\sigma(x) \\approx 1\\) or \\(\\sigma(x) \\approx 0\\), and the gradient \\(\\frac{\\partial \\sigma}{\\partial x} \\approx 0\\). The gate becomes frozen.\nSwish doesnâ€™t saturate for positive inputs, it grows approximately linearly (just like ReLU). This implies that: - Gradients flow better through the gate path - The gate can modulate rather than just switch on/off\n3. smoothness\nAnother thing is that SwiGLU is infinitely differentiable & this smoothness likely helps optimization stability.\n\n\n\nFinal note\nSwiGLUâ€™s power comes from its gating mechanism & multiplicative interactions. By splitting the input into two paths and multiplying them, the network can learn which feature combinations matter (which is quite similar to how attention captures interactions through \\(QK^T\\)).\nCombined with Swishâ€™s non-saturating gradients, this makes SwiGLU particularly effective for large models."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Weekends Eureka",
    "section": "",
    "text": "Why does SwiGLU work ?\n\n\n\nPython\n\nNLP\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nJan 14, 2026\n\n\nSafouane Chergui\n\n\n\n\n\n\n\n\n\n\n\n\nStop Vibe-Checking: Real-World Lessons on LLM Evals\n\n\n\nLLM\n\nAgent\n\nEvaluation\n\n\n\n\n\n\n\n\n\nNov 19, 2025\n\n\nSafouane Chergui\n\n\n\n\n\n\n\n\n\n\n\n\nPDF parsing is hard\n\n\n\nPython\n\nPDF\n\n\n\n\n\n\n\n\n\nAug 29, 2025\n\n\nSafouane Chergui\n\n\n\n\n\n\n\n\n\n\n\n\nWhy I ditched pip and conda for Pixi\n\n\n\nPython\n\nPackage management\n\n\n\n\n\n\n\n\n\nJul 5, 2025\n\n\nSafouane Chergui\n\n\n\n\n\n\n\n\n\n\n\n\nByte Pair Encoding Tokenization\n\n\n\nPython\n\nNLP\n\n\n\n\n\n\n\n\n\nJul 7, 2024\n\n\nSafouane Chergui\n\n\n\n\n\nNo matching items"
  }
]