[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog serves for archiving my notes."
  },
  {
    "objectID": "posts/Byte Pair Encoding Tokenization/Byte Pair Encoding Tokenization.html",
    "href": "posts/Byte Pair Encoding Tokenization/Byte Pair Encoding Tokenization.html",
    "title": "Byte Pair Encoding Tokenization",
    "section": "",
    "text": "The aim of this blog is to explain to you how BPE Tokenization works. We’re going to build a basic tokenizer using BPE tokenization and we’ll apply it on a dummy example.\nWe’re not going to go into the gory details of it. If you’d like a great source for the details of tokenization, watch this 2-hour long video by the amazing Andrej Karpathy. Everything in this post comes from Andrej’s video or the resources he shared in his video.\nTable of contents\n- Hexadecimal representation\n- UTF-8 encoding\n- Some Unicode vocab\n- WTF is UTF-8 ?\n- A sentence example\n- BPE Tokenization\n- BPE in action\n- How does it work\n- Let us build a BPE tokenizer\n- Encoding\n- Decoding\n- One last problem to solve\n- One final example"
  },
  {
    "objectID": "posts/Byte Pair Encoding Tokenization/Byte Pair Encoding Tokenization.html#hexadecimal-representation",
    "href": "posts/Byte Pair Encoding Tokenization/Byte Pair Encoding Tokenization.html#hexadecimal-representation",
    "title": "Byte Pair Encoding Tokenization",
    "section": "Hexadecimal representation",
    "text": "Hexadecimal representation\nLet go for a quick tour of hexadecimal representation. We, as humans, usually represent numbers in base 10. Computers make use of base 2 representation.\nHexadecimal representation means representing a number in base 16. Numbers are represented using 0 to 9 and A-F, with A being equivalent to 10, B to 11, …, F to 15.\nThe number 125 can be written in hexadecimal as 7D.\n\n# d * (16**0) + 7 * (16**1) = 13 + 7 * 16\nhex(125)\n\n'0x7d'\n\n\nNow, a byte is represented with 8 bits, with a range going from 00000000 to 11111111 &lt;-&gt; 0 to 255 in decimal &lt;-&gt; 00 to FF in hexadecimal\n\nprint(hex(255))\n\n0xff\n\n\nNote: The prefix 0x is used in intergers to tell Python to interpret the number as a hexadecimal. For example:\n\n# Python understands that this number is represented in hexadecimal\nhex_number = 0xff\n\nprint(hex_number)\n\n255"
  },
  {
    "objectID": "posts/Byte Pair Encoding Tokenization/Byte Pair Encoding Tokenization.html#utf-8-encoding",
    "href": "posts/Byte Pair Encoding Tokenization/Byte Pair Encoding Tokenization.html#utf-8-encoding",
    "title": "Byte Pair Encoding Tokenization",
    "section": "UTF-8 encoding",
    "text": "UTF-8 encoding\nLittle background about UTF-8 encoding. So, UTF-8 is an encoding that represents Unicode characters using 1 to 4 bytes. So, every string is converted to a series of bytes once encoded using a Unicode encoding (such as UTF-8).\n\nSome Unicode vocab\nYou should know at least these two words when dealing with Unicode: character and code point: - A character is a textual unit in Unicode. It can be anything ranging from a letter, a digit, an emoji, a mathematical symbol, etc.\n\ncharacter = \"A\"\nprint(f\"Character: {character}\")\n\n# First letter of my name in Arabic\n# You might think of it as S\n# Arabic speakers, don't insult me for this analogy \narabic_s_character = \"ص\"\nprint(f\"Arabic character: {arabic_s_character}\")\n\nCharacter: A\nArabic character: ص\n\n\n\nA code point is a unique number assigned to each character in Unicode. You can obtain it in Python using the ord function.\n\n\nA_code_point = ord(character)\nprint(f\"The character {character} has as a code point {A_code_point} in decimal, which corresponds to {hex(A_code_point)} in hexadecimal\")\n\narabic_s_code_point = ord(arabic_s_character)\nprint(f\"The character {arabic_s_character} has as a code point {arabic_s_code_point} in decimal, which corresponds to {hex(arabic_s_code_point)} in hexadecimal\")\n\nThe character A has as a code point 65 in decimal, which corresponds to 0x41 in hexadecimal\nThe character ص has as a code point 1589 in decimal, which corresponds to 0x635 in hexadecimal\n\n\nNice to know: - Use ord to go from a character -&gt; a code point - Use chr to go from a code point -&gt; a character\nAt this step, I highly advise you to take a look at the start of the Unicode Character Table.\nIf you look at this table, you’ll see that the character ص doesn’t have 1589 as a code point, it rather has U+0635. These are the same number, it’s just that Unicode uses the hexadecimal notation for code points instead of the decimal one.\n\n\nWTF is UTF-8 ?\nSo, UTF-8 is a Unicode encoding method. It encodes code points in 1 to 4 bytes (it is a variable-length encoding, not all code points are encoded as 4 bytes as this will consume so much memory!).\nOther encoding methods such as UTF-16 and UTF-32 exist but they do things differently than UTF-8. UTF-8 is the most used encoding method in the wild for reasons that I won’t go into in this post.\nNow, the thing is that UTF-8 encoding doesn’t just convert the Unicode code point to its binary presentation. It does it in a specific way. The following table from wikipedia explains very well the conversion from code points to UTF8-encoding.\nThis table shows four ranges, the 1st range is encoded using 1 byte only, the second one 2 bytes, etc. Each range has its own norm for encoding code points using UTF-8.\n\n\n\nCharacter ص example:\nThe character ص code point is U+0635 (1589 in decimal). If you try to represent this using binary presentation, it doesn’t fit into 1 byte, you’ll need to use 2 bytes.\nUsing 2 bytes, it yields the following 00000110 00110101. Now, UTF-8 doesn’t use conversion to bytes to encode code points. It has its own rules to do the encoding.\nNow, if you look at the table, you see that U+0635 falls between the range of U+0080 and U+07FF. So, we have to use the rule in the second row to encode it using UTF-8.\nThe way to do this is to fill the x with the numbers in the binary representation starting from the right (copy-paste digit from the binary representation starting from the right): - Before filling: [110]xxxxx [10]xxxxxx - After filling: [110]11000 [10]110101. In hexadecimal, this yields d8 b5.\n\nprint(f\"The byte 11011000 yields in hexadecimal: {hex(0b11011000)}\")\nprint(f\"The byte 10110101 yields in hexadecimal: {hex(0b10110101)}\")\n\nThe byte 11011000 yields in hexadecimal: 0xd8\nThe byte 10110101 yields in hexadecimal: 0xb5\n\n\nNow, this should match the result of the UTF-8 encoding of the character ص using Python\n\n\"ص\".encode(\"utf-8\")\n\nb'\\xd8\\xb5'\n\n\nThe same rule can be applied to all the 4 ranges in order to go from a code point to its UTF-8 encoding.\nAt this step, I suggest you pick some characters from the unicode table page I’ve given you above and do the encoding manually.\nNote: There is something that you should note at this step: - Unicode allows us to go from a character to a code point - The UTF-8 encoding (which is one of Unicode available encodings) allows us to go from a code point to a 1 to 4 bytes representation, depending on the character.\nThis yields the following trajectory: Unicode character &lt;-&gt; Code point -&gt; 1 to 4 bytes representation.\nNow, if you take any 1 to 4 bytes number, it doesn’t always map to a code point as it might not respect the rules specified in the wikipedia table above. For example, 11111111 (255 in decimal) corresponds to 1 byte but it’s not a valid unicode encoding. This is because the rule specifies that all 1-byte encoded characters should start with a 0 and not a 1.\n\n\nA sentence example\nLet us now encode a whole sentence and see the result: Hello there صفوان\n\nex_sentence = \"Hello there صفوان\"\n\nprint(f\"The UTF-8 encoding of the sentence '{ex_sentence}' is: {ex_sentence.encode('UTF-8')}\")\n\nThe UTF-8 encoding of the sentence 'Hello there صفوان' is: b'Hello there \\xd8\\xb5\\xd9\\x81\\xd9\\x88\\xd8\\xa7\\xd9\\x86'\n\n\nI hope you at least recognize the UTF-8 encoding of the character ص: \\xd8\\xb5\nA question that might come to your mind upong seeing the result is why the hell we have characters like H in a bytes representation. This is something that has to do with the display of bytes in Python.\nPython uses a mixed representation of bytes for readability: - ASCII characters, even when encoded as bytes, are displayed in their readable form - Non-ASCII characters, when encoded as bytes, are displayed in hexadecimal format\nIt’s just a displaying matter !"
  },
  {
    "objectID": "posts/Byte Pair Encoding Tokenization/Byte Pair Encoding Tokenization.html#bpe-tokenization",
    "href": "posts/Byte Pair Encoding Tokenization/Byte Pair Encoding Tokenization.html#bpe-tokenization",
    "title": "Byte Pair Encoding Tokenization",
    "section": "BPE Tokenization",
    "text": "BPE Tokenization\nIn order to feed data to a model, we have to first convert it to numerical data. The process for NLP models looks as follow:\n\nText —&gt; Tokenizer —&gt; Numerical ids (tokens) —&gt; NLP model —&gt; Prediction\n\nNow, if you look at the definition of str in Python, it’s defined as follows:\nTextual data in Python is handled with str objects, or strings. Strings are immutable sequences of Unicode code points\nFrom this definition, you can decide to just map every unicode character to its code point. For e.g, ص will be mapped to 1589. Using this technique will yield a vocabulary of almost 150 000 tokens.\nThe problem with this is: - Your vocabulary will be big. This means that your model will have embedding layers of 150 000 * size_of_your_embeddding - With that many characters, it’s very likely that your model training dataset won’t contain many of them that often, if not at all. If your model gets to see ص very few times, its embedding will be very ill trained. At the end of training, its embedding will be very close to its random initialization state and would be of no use at all. - As your input sentence will be divided in unique characters, a simple sentence will consume lots of tokens in your input context. This means that: - At inference, you’ll have to make predictions for every character - Your sentence will take so much memory because even a simple sentence will be long, many embeddings used and calculations will have to be made over all those embeddings - Simple text will consume so much of the model context size\nLet look at this famous sentence The quick brown fox jumps over the lazy dog. If every character is considered a token, it will consist of 28 different tokens\n\nexample_sentence= \"The quick brown fox jumps over the lazy dog\"\nprint(f\"Number of tokens in {example_sentence} is: {len(set([e for e in example_sentence]))}\")\n\nNumber of tokens in The quick brown fox jumps over the lazy dog is: 28\n\n\nIf we use GPT-4o tokenizer that is a BPE tokenizer itself, let us see how many tokenizer we get:\n\nimport tiktoken\n\ntokenizer = tiktoken.get_encoding(\"o200k_base\")\nencoded_input = tokenizer.encode(example_sentence)\nprint(f\"Number of tokens in {example_sentence} with GPT-4o tokenizer is: {len(encoded_input)}\")\n\nNumber of tokens in The quick brown fox jumps over the lazy dog with GPT-4o tokenizer is: 9\n\n\nWhen we look at the tokens resulting from GPT-4o tokenizer, we get to see that every word is a token in this case.\n\nfor token in encoded_input:\n    print(f\"The token with id {token} represents {tokenizer.decode_single_token_bytes(token)}\")\n\nThe token with id 976 represents b'The'\nThe token with id 4853 represents b' quick'\nThe token with id 19705 represents b' brown'\nThe token with id 68347 represents b' fox'\nThe token with id 65613 represents b' jumps'\nThe token with id 1072 represents b' over'\nThe token with id 290 represents b' the'\nThe token with id 29082 represents b' lazy'\nThe token with id 6446 represents b' dog'\n\n\nIf you’d like to play with different tokenizers, visit this page: https://tiktokenizer.vercel.app/, select the tokenizer and get going!\n\nBPE in action\nFor this, you’ll need to install tiktoken package using pip install tiktoken.\n\nimport tiktoken\n\ntokenizer = tiktoken.get_encoding(\"o200k_base\")\n\n\ninput_str = \"My name is صفوان\"\nencoded_input = tokenizer.encode(input_str)\nprint(f\"The result of tokenization consists of {len(encoded_input)} tokens: {encoded_input}\")\n\nThe result of tokenization consists of 5 tokens: [5444, 1308, 382, 37315, 10878]\n\n\nLet us decode each one of these tokens and see what they consist of:\n\nfor token in encoded_input:\n    print(f\"The token with id {token} represents {tokenizer.decode_single_token_bytes(token)}\")\n\nThe token with id 5444 represents b'My'\nThe token with id 1308 represents b' name'\nThe token with id 382 represents b' is'\nThe token with id 37315 represents b' \\xd8\\xb5\\xd9\\x81'\nThe token with id 10878 represents b'\\xd9\\x88\\xd8\\xa7\\xd9\\x86'\n\n\nI hope you recognize, in the 4th print statement, the hexadecimal representation of the character ص We can see that the fourth\n\n[tokenizer.decode_single_token_bytes(x) for x in encoded_input]\n\n[b'My', b' name', b' is', b' \\xd8\\xb5\\xd9\\x81', b'\\xd9\\x88\\xd8\\xa7\\xd9\\x86']\n\n\nLet us decode that fourth element and see what it consists of:\n\ntokenizer.decode_single_token_bytes(encoded_input[3]).decode(\"utf-8\")\n\n' صف'\n\n\nWe can see that this token consists of 3 unicode characters: - The space character - ص, the character with the code point U+0635, whose UTF-8 encoding is d8 b5 - ف, the character with the code point U, whose UTF-8 encoding is d9 81\n\n\nHow does it work\nAs you might have guessed from its name, the Byte Pair Encoding mechanism does mainly two things to tokenize your data: - It represents strings as streams of bytes. Now, if you represent your data using only bytes, as a character usually consists of many bytes, that will be worse than using unicode code points as tokenized strings will be very very long. A byte can go only from 0 to 255.\n\nWhen some pair of bytes are so common in the corpus, it’s going to merge them going forward and assign a new id to them. So, in the example of the token ’ صف’, GPT-4 tokenizer has decided to merge the bytes that these two arabic characters consist of, meaning these four bytes:\n\nspace character (20 in hex, 32 in decimal)\nd8 (216 in decimal)\nb5 (181 in decimal)\nd9 (217 in decimal)\n81 (129 in decimal)\n\n\nand assigned to the whole token '  صف' the id 37315.\nOf course, the vocabulary identified by tokenizers can’t just grow indefinitly. otherwise, your model embedding table will also be huge and the model will be practically unusable. That’s why BPE tokenizers have a hyperparameter that specifies the number of merges that can be done when training.\nA quick recap: When training a tokenizer, convert string to bytes. The result will consist of numbers going from 0 to 255. Merge bytes that come-up together so often and assign an id to them.\n\n\nLet us build a BPE tokenizer\n\nEncoding\nSo, we basically need to do three things to build our tokenizer: - Define our basic vocab that we’ll grow as we merge the most common pair of bytes - Convert our corpus to a stream of bytes (just encoding it using UTF-8) - Do the following number_of_merges times: - Get the most common pair of bytes - Merge the two pairs in a new id. Everytime the tokenizer will see the two pairs together, it will map them to the new ID.\nSo, our starting vocab is just the 1 byte interval (0 to 255), and as we merge the most common, we’ll append new ids corresponding to the merged bytes.\n\nvocab = {i:i for i in range(256)}\n\nAs stated before, the first step is to convert our corpus to a stream of bytes by encoding using UTF-8.\nLet us start with a very basic training corpus, it will make testing our functions very easy. Later, we’ll use a corpus consisting of many languages.\n\ncorpus = \"Hello Hello Hello my name is Safouane and I am the author of this post\"\n\n\nencoded_corpus = corpus.encode(\"UTF-8\")\n\nAs we want to use bytes using the decimal representation (0-255) instead of hexadecimal (0x00 - 0xFF) (the default after encoding with UTF-8) for readability purposes, we’ll do the conversion right away\n\nencoded_corpus = [int(x) for x in encoded_corpus]\nprint(encoded_corpus[:10])\n\n[72, 101, 108, 108, 111, 32, 72, 101, 108, 108]\n\n\nLet build a function that returns the most common pair of bytes in the whole corpus\n\nfrom collections import Counter\n\ndef get_most_common_pair(encoded_corpus: list[int]) -&gt; tuple:\n    \"\"\"Returns a tuple of the most common pair of bytes in a corpus.\n    \"\"\"\n    most_common_pair_and_count = Counter(zip(encoded_corpus[:-2], encoded_corpus[1:])).most_common(1)\n    most_common_pair = most_common_pair_and_count[0][0]\n    return most_common_pair\n\nLet us see what’s the most common pair of bytes in this corpus\n\nmost_common_pair = get_most_common_pair(encoded_corpus)\n\nprint(f\"The most common pair is {most_common_pair}. {most_common_pair[0]} corresponds to {chr(most_common_pair[0])} and {most_common_pair[1]} corresponds to {chr(most_common_pair[1])}\")\n\nThe most common pair is (72, 101). 72 corresponds to H and 101 corresponds to e\n\n\nLet us write a function that takes the encoded corpus, the vocabulary, the pair of ids to merge and: - Updates the vocabulary by adding the new id that replaces the occurence of the pair of the ids to merge - Updates the corpus by replacing the occurence of the pair of ids with the new id\n\ndef merge_and_update_corpus_and_vocab(encoded_corpus, vocab, pair_to_merge):\n    \"\"\"Updates the vocab with the new pair of ids to merge\n    & updates the corpus to use the new id instead of the pair of bytes\n    \"\"\"\n    updated_corpus = []\n    updated_vocab = vocab.copy()\n\n    # Add id for merged pair in vocab\n    pair_new_id = max(vocab.keys()) + 1\n    updated_vocab[pair_new_id] = pair_to_merge\n\n    # Update corpus to use the new id instead of the couple the pair of ids\n    i = 0\n\n    while i &lt; len(encoded_corpus):\n        if (i &lt; len(encoded_corpus) - 1) and encoded_corpus[i] == pair_to_merge[0] and encoded_corpus[i + 1] == pair_to_merge[1]:\n            updated_corpus.append(pair_new_id)\n            i += 2\n        else:\n            updated_corpus.append(encoded_corpus[i])\n            i +=1\n    \n    return updated_corpus, updated_vocab\n\nLet us merge now the most common pair of bytes and create one id out of it. The successive 101,32 will be turned into 256.\n\nupdated_corpus_ex, updated_vocab_example = merge_and_update_corpus_and_vocab(\n    encoded_corpus, \n    vocab,\n    get_most_common_pair(encoded_corpus)    \n)\n\nWe can see the presence of a new token id in the corpus: 256\n\nprint(updated_corpus_ex[:10])\n\n[256, 108, 108, 111, 32, 256, 108, 108, 111, 32]\n\n\nWhen we check the vocab to see to what it corresponds, we see that it’s a merge of two ids: 72 and 101.\n\nprint(updated_vocab_example[256])\n\n(72, 101)\n\n\nWe’ve seen how merging works. Now, we have to apply this merging a number_of_merges times. What you should keep in mind is the following: - You vocab will grow with the number of merges you apply. Say you apply 10 merges, your vocab will go from 256 ids to 266 ids. - The bigger your vocab gets, the bigger your embedding table (whose size corresponds to number of tokens in the vocab * size of embedding) will grow. - Merging indefinitly is not a good idea. The different ids in your vocab will not come up often as tokens in the training corpus of your model. Consequently, some tokens will have random embeddings - Having merged tokens allows you to make good use of your context size as your tokens will can even represent complete words at times\n\ndef merge_iteratively(corpus, vocab, number_of_merges: int):\n    for _ in range(number_of_merges):\n        most_common_pair = get_most_common_pair(corpus)\n        corpus, vocab = merge_and_update_corpus_and_vocab(corpus, vocab, most_common_pair)\n    \n    return corpus, vocab\n\n\nthree_merges_corpus, three_merges_vocab = merge_iteratively(encoded_corpus, vocab, 3)\n\nprint(f\"A glimpse at the start of new corpus: {three_merges_corpus[:10]}\")\n\nprint(f\"The merged ids: {[three_merges_vocab[255 + x] for x in range(1, 4)]}\")\n\nA glimpse at the start of new corpus: [258, 111, 32, 258, 111, 32, 258, 111, 32, 109]\nThe merged ids: [(72, 101), (256, 108), (257, 108)]\n\n\nNow, the merge_iteratively function if it’s out of pairs that occur more than once, it will start merging pairs that occur only once. This is not a desired behavior.\nThis can be handled easily by returning the count of the number of occurences as well in get_most_common_pair and checking that it’s greater than 1 to proceed to merging.\n\ndef get_most_common_pair(encoded_corpus: list[int]) -&gt; tuple:\n    \"\"\"Returns a tuple:\n    - A tuple of the most common pair of ids in a corpus\n    - The number of occurences of the pair of ids\n    \"\"\"\n    most_common_pair_and_count = Counter(zip(encoded_corpus[:-2], encoded_corpus[1:])).most_common(1)[0]\n    return most_common_pair_and_count\n\n\ndef merge_iteratively(corpus, vocab, number_of_merges: int):\n    for _ in range(number_of_merges):\n        most_common_pair, count = get_most_common_pair(corpus)\n        if count == 1:\n            break\n        else:\n            corpus, vocab = merge_and_update_corpus_and_vocab(corpus, vocab, most_common_pair)\n    \n    return corpus, vocab\n\nLet put all these functions in one function that should do the encoding of a string\n\ndef encode(corpus: str, num_merges: int):\n    encoded_corpus = corpus.encode(\"UTF-8\")\n    base_vocab = {i:i for i in range(256)}\n    updated_corpus, updated_vocab = merge_iteratively(encoded_corpus, base_vocab, num_merges)\n    return updated_corpus, updated_vocab\n\nNow that we can train a tokenizer and encode a string, all that’s left is decoding.\n\n\nDecoding\nNow, to detokenize tokenized text, it’s not that hard - First, you have to reverse the mapping. You have to unpair the ids. If for example, the id 257=(100,256) and 256=(255, 20), we have to unpair those. But in order for the unpairing to work, we have to unpair the ids starting from the highest id, 257 in this case. 257 will be replaced with (100 and 256) and then 256 can be replaced with (255 and 20). If you don’t do it in the right order, you’ll have some composed ids left and you won’t be able to decode them. - Once the mapping is reversed, you can decode the stream of bytes using UTF-8\n\nthree_merges_corpus, three_merges_vocab = encode(corpus, 3)\n\nLet us first build a function that will help us get the pair of ids that were replaced with a new id\n\ndef unpair_ids(corpus, vocab):\n    \"\"\"Reverses new ids in the vocab to their\n    orignal pair of ids.\n    \"\"\"\n    for token in sorted(vocab.keys(), reverse=True):\n        if token &gt; 255:\n            unpaired_corpus = []\n            for i in range(len(corpus)):\n                if corpus[i] == token:\n                    unpaired_corpus.append(vocab[token][0])\n                    unpaired_corpus.append(vocab[token][1])\n                else:\n                    unpaired_corpus.append(corpus[i])\n                \n            corpus = list(unpaired_corpus)\n    \n    return unpaired_corpus\n\n\nprint(f\"The first ten tokens of the tokenized corpus looks like: {three_merges_corpus[:10]}\")\nprint(f\"The first ten tokens of the unpaired corpus looks as follows: {unpair_ids(three_merges_corpus[:10], three_merges_vocab)}\")\n\nThe first ten tokens of the tokenized corpus looks like: [258, 111, 32, 258, 111, 32, 258, 111, 32, 109]\nThe first ten tokens of the unpaired corpus looks like: [72, 101, 108, 108, 111, 32, 72, 101, 108, 108, 111, 32, 72, 101, 108, 108, 111, 32, 109]\n\n\nNow that we recovered the corpus with the bytes ids, we can do the UTF-8 decoding:\n\ndef decode(tokenized_corpus_with_merges, vocab_with_merges):\n    unpaired_tokenized_corpus = unpair_ids(tokenized_corpus_with_merges, vocab_with_merges)\n    # The UTF-8 decoding expects byte objects and not int, we have to convert them to byte first\n    decoded_corpus = b\"\".join([x.to_bytes() for x in unpaired_tokenized_corpus]).decode(\"UTF-8\")\n    return decoded_corpus\n\n\ndecoded_corpus = decode(three_merges_corpus, three_merges_vocab)\nprint(f\"The decoding of the corpus yields: {decoded_corpus}\")\nprint(f\"Check of equality with the original corpus: {decoded_corpus == corpus}\")\n\nThe decoding of the corpus yields: Hello Hello Hello my name is Safouane and I am the author of this post\nCheck of equality with the original corpus: True\n\n\n\n\nOne last problem to solve\nWhen encoding, we get to add new ids as tokens and we just incremented with 1 each time. Now, remember this table from wikipedia:\n\n\n\nWhat this table entails is that not all bytes are valid UTF-8 encodings. For example, any 1-byte number starting with 1 its binary representation is not a valid UTF-8 encoding. The number 10000000 (which corresponds to 128 in the decimal representation) is not a valid UTF-8 encoding.\nLet us check what Python will say if we try to decode it using UTF-8\n\n# Convert the int to a bytes object\nexample_byte = 0b10000000.to_bytes()\n\n# Decode it using UTF-8\nexample_byte.decode(\"UTF-8\")\n\n\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\nCell In[215], line 5\n      2 example_byte = 0b10000000.to_bytes()\n      4 # Decode it using UTF-8\n----&gt; 5 example_byte.decode(\"UTF-8\")\n\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n\n\n\nThe error says invalid start byte, can’t be any clearer !\nNow, what should we do if we encounter such integers in our encoded corpus ? The way this is circumvented in GPT-4 tokenizer for example is by decoding that integer into a specific character �. This is the default value that any invalid integer is decoded into with decode method using UTF-8 encoding in Python.\n\nexample_byte.decode(\"UTF-8\", errors=\"replace\")\n\n'�'\n\n\n\n\nOne final example\nAs an example of our training corpus, we’ll take the description in english, french and arabic of 1337, one of the leading IT schools in the world that is located in Morocco. This description will constitute our corpus that we’ll use to train our tokenizer.\n\ninput_string = \"\"\"1337 is the first to provide IT training in Morocco, completely free of charge, open and accessible to anyone between the ages of 18 and 30. No need for an IT degree, or of having undergone extensive IT training. The only criteria for admission in Treize, Trente-Sept is CREATIVITY.\n\nThe Treize, Trente-Sept educational approach is based on peer-learning. A participatory operating style allowing students to unleash their creativity through project-based learning. To train the future coders of tomorrow, we had to rethink learning. We had to make IT something fun, exciting and at odds with the restrictive vision that the general public may have about it.\n\n1337 is the coding school par excellence, completely free of charge and accessible to all with no prerequisite of a degree. It offers a full immersion in a universe where the future is already present. Where IT and the lines of code are way more than a vague and off-putting concept…\n\nTreize, Trente-Sept, a forward-looking school from the present.\n1337, c’est la première formation en informatique par excellence au Maroc, entièrement gratuite, ouverte 24h/24 7j/7 et accessible à tous sans pré-requis de diplôme, ou de connaissance en informatique.. C’est une immersion complète dans un univers où le futur est déjà présent, où l’informatique et les lignes de codes sont plus qu’un concept flou et rébarbatif…\n\nLa pédagogie de Treize, Trente-Sept s’articule autour du peer-learning. Un fonctionnement participatif qui permet aux étudiants de libérer leur créativité grâce à l’apprentissage par projet. Pour former les futurs codeurs de demain, il fallait repenser l’apprentissage, faire de l’informatique quelque chose de ludique, de passionnant et aux antipodes de la vision restrictive que le grand public peut en avoir.\n\nTreize, Trente-Sept utilise les méthodes techniques et pédagogiques de 42 Paris, élue meilleure école de code au monde par CodinGame.\n\nTreize, Trente-Sept, une école du présent tournée vers le futur.\n1337 هي أول تكوين معلوماتي فالمغرب، كلو بالمجان، مفتوح للجميع اللي تيتراوح عمرهم بين 18 و30 سنة. ما محتاجش يكون عندك دبلوم فالمعلوميات، أو تكون درتي شي تكوين أو تخصص فالمعلوميات. المفتاح الوحيد اللي كيتيح لك باب الدخول ل 1337 هو الإبداع.\n\nالبيداغوجية ديال 1337 كتعتمد على peer-learning اللي هو نوع من التعلم التعاوني اللي كيساعد الطلبة على تحرير الإبداع ديالهم بفضل التعلم بإنجاز المشاريع. وباش نكوّنو مبرمجي الغد، اللي كيكوديو، كان لازم نراجعو طريقة التعلم، ونجعلو تعلم المعلوميات عملية ترفيهية، فيها الرغبة والشغف، ماشي كيف كيتخيلوها الناس.\n\n1337 هي مدرسة الكود بامتياز، مجانية وفمتناول الجميع, وما تتطلبش منك تكون حائز على دبلوم. وتعتبر اندماج كامل، فعالم مستقبلي اللي ما بقاش فيه الكود مفهوم غامض.\n\n1337 مدرسة الحاضر المتطلعة للمستقبل.\n\"\"\"\n\n\nencoded_merged_input, updated_vocab = encode(input_string, 20)\n\nThe added ids to the corpus are the following:\n\nprint({x:updated_vocab.get(x) for x in range(256, max(updated_vocab.keys()) + 1)})\n\n{256: (101, 32), 257: (217, 132), 258: (216, 167), 259: (217, 133), 260: (217, 138), 261: (217, 136), 262: (32, 216), 263: (258, 257), 264: (114, 101), 265: (115, 32), 266: (105, 110), 267: (116, 32), 268: (32, 263), 269: (216, 170), 270: (44, 32), 271: (217, 134), 272: (216, 185), 273: (116, 105), 274: (217, 131), 275: (101, 114)}\n\n\n\ndecoded_corpus = decode(encoded_merged_input, updated_vocab)\n\nLet us check the decoded corpus against the original corpus:\n\ndecoded_corpus == corpus\n\nTrue\n\n\nWhile in this case the result of the decoding process yields the same result as the original, you shouldn’t always expect it to be the case. If during the decoding process, we encounter an id (like 128) that we can’t decode using UTF-8, it will get replaced with � in the current implementation. The comparison of the decoding result with the original text won’t yield equality."
  },
  {
    "objectID": "posts/Pixi intro/Why I ditched conda & pip for Pixi.html",
    "href": "posts/Pixi intro/Why I ditched conda & pip for Pixi.html",
    "title": "Why I ditched pip & conda for Pixi",
    "section": "",
    "text": "I’ve been using pip and then conda for as long as I can remember. Last year, I did a double-switch. First, I moved to uv and then not long after it I moved to pixi.\nI discovered pixi thanks to Eric Ma blog post. At the time, the thing that caught my attention the most is how easy it is to manage the installation of the same environment but one with CUDA support and the other without.\nAfter months of using pixi now, I can say that the 3 things I like the most about pixi are: - The features concept that allows to mix and match packages to create environments - Being able to run tasks - How fast it is!\nWe’ll take a look at all of this in this blog. The final version of the code generated in this blog is available in this repository."
  },
  {
    "objectID": "posts/Pixi intro/Why I ditched conda & pip for Pixi.html#pixis-toolset",
    "href": "posts/Pixi intro/Why I ditched conda & pip for Pixi.html#pixis-toolset",
    "title": "Why I ditched pip & conda for Pixi",
    "section": "Pixi’s toolset",
    "text": "Pixi’s toolset\nNow, Pixi is many things but I’ll focus on the things that will be of use to you as a python developer\n\nPixi is a package manager that can manage packages from both Conda & PyPI. The dependency resolution tools used by Pixi (resolvo for conda & uv resolution tool for PyPI packages) are very fast.\nPixi manages environments (similar to venv for pip users, a feature that is built into conda)\nPixi manages python version as well (similar to pyenv if you use pip, built into conda)\nPixi has a lock file that allows you to reproduce excatly the same environment (similar to what you’d get conda-lock or pip-lock)\nPixi can be used as a task-runner, just like make or just.\nPixi has built-in cross-platform reproducibility. The lock file includes the exact versions and dependencies in all targeted platforms. You can pick and choose the targeted platform by your project (Linux, Windows, etc).\nPixi can also install tools like brew and you can have access to the globally.\n\nNow, while mamba is fast, in my experience, pixi is faster. mamba also lacks lock-files that are essential for reproducibility and a task runner that comes very handy in many situations (CI/CD, Other people running your project, etc.)"
  },
  {
    "objectID": "posts/Pixi intro/Why I ditched conda & pip for Pixi.html#pixis-project-philosophy",
    "href": "posts/Pixi intro/Why I ditched conda & pip for Pixi.html#pixis-project-philosophy",
    "title": "Why I ditched pip & conda for Pixi",
    "section": "Pixi’s project philosophy",
    "text": "Pixi’s project philosophy\nWhile conda is environment-centric, pixi is all about projects. When you init a pixi project, it will create a pixi.toml (or a pyproject.toml instead if you want). In this file, you can specify many environments that can be composed of different features. For example, you can have: - a base feature that includes the basic packages needed by your project - a run feature that consists of packages needed for only running the project - a test feature that consists of additional packages needed for testing the project - a build feature that consists of additional packages or tools needed for building the project.\nImagine having to train a model on a GPU but then when running it, to only have a CPU at your disposal. What you would do is have: - A training environment composed of the features base + build + test that will include the base packages, some CUDA dependencies and pytorch with GPU support coming from the build feature, and test packages like pytest coming from the test feature. - A CI/CD environment composed of base + run + test. The only difference this time is that you’ll be using the run feature that include pytorch-cpu and no CUDA dependencies. - A run environment composed only of base + run features.\nThe other nice thing is that you can say that you can enforce that some environments (or all of them) use the same versions of the common packages."
  },
  {
    "objectID": "posts/Pixi intro/Why I ditched conda & pip for Pixi.html#installation",
    "href": "posts/Pixi intro/Why I ditched conda & pip for Pixi.html#installation",
    "title": "Why I ditched pip & conda for Pixi",
    "section": "Installation",
    "text": "Installation\nStart first by installing pixi by grabbing the one command-line that corresponds to your case from here: Pixi installation.\nIt’s really just one command, restart your terminal and there you go."
  },
  {
    "objectID": "posts/Pixi intro/Why I ditched conda & pip for Pixi.html#getting-hands-on",
    "href": "posts/Pixi intro/Why I ditched conda & pip for Pixi.html#getting-hands-on",
    "title": "Why I ditched pip & conda for Pixi",
    "section": "Getting hands-on",
    "text": "Getting hands-on\n\nInitiating the project\nWe’ll work through an example where we’d like to develop a FastAPI app.\nLet us initiate a pixi project:\npixi init fastapi_app --format pyproject\ncd fastapi_app\nIf you already have an existing folder, you can simply go inside of it and execute\npixi init --format pyproject\nBy default, pixi uses a pixi.toml file for its configuration. As people in python use pyproject.toml, you can specify that you want to use the latter with the --format pyproject.\nThe initiation of the project creates the following files:\n\n\n\nIf you look at the content of the pyproject.toml, you’ll see different sections:\n[project]\nauthors = [{name = \"Safouane Chergui\", email = \"chsafouane@gmail.com\"}]\nname = \"fastapi_app\"\nrequires-python = \"&gt;= 3.11\"\nversion = \"0.1.0\"\ndependencies = [ \"fastapi&gt;=0.115.14,&lt;0.116\", \"uvicorn[standard]&gt;=0.35.0,&lt;0.36\"]\n\n[build-system]\nbuild-backend = \"hatchling.build\"\nrequires = [\"hatchling\"]\n\n[tool.pixi.workspace]\nchannels = [\"conda-forge\"]\nplatforms = [\"win-64\"]\n\n[tool.pixi.pypi-dependencies]\nfastapi_app = { path = \".\", editable = true }\n\n[tool.pixi.tasks]\nLet us dive into the most important fields:\nThe [project] section includes project metadata. - As we haven’t added a specific python interpreter to the project, the requires-python entry shows the currently active python interpreter in the terminal. You can change it manually if you want.\nThe [tool.pixi.workspace] section has two entries: - The channels shows the conda channels that can be used to download the conda packages.If you have a company repository (like nexus), it can be used instead or added before conda-forge to be used first. - The platforms corresponds to the platform you’re using. You can add other platforms here and the pixi.lock will include the packages that need to be installed to reproduce the exact environment in the case of the additional platforms.\nThe [tool.pixi.pypi-dependencies] section is used to specify the packages to install from PyPI. By default, the code you’re developping shows up as an editable package. Your code will be installed in editable mode and you’ll be able to see the changes you make to your code directly reflected in your environment.\nThe [tool.pixi.tasks] section is empty for the time-being. You can imagine tasks as a replacement of makefiles. We’ll add some tasks later in the blog post.\n\n\nAdding dependencies\nLet us add python 3.12 to the project\npixi add python=3.12\nAs we’re going to create a FastAPI app, let us add fastapi and uvicorn but this time from PyPI.\npixi add --pypi fastapi \"uvicorn[standard]\"\nNow that we have proceeded with adding these dependencies, we can see that we have a pixi.lock file that was created.\n\n\n\nThe pyproject.toml file is now updated to include the new dependencies:\n[project]\nauthors = [{name = \"Safouane Chergui\", email = \"chsafouane@gmail.com\"}]\nname = \"fastapi_app\"\nrequires-python = \"&gt;= 3.11\"\nversion = \"0.1.0\"\ndependencies = [ \"fastapi&gt;=0.115.14,&lt;0.116\", \"uvicorn[standard]&gt;=0.35.0,&lt;0.36\"]\n\n[build-system]\nbuild-backend = \"hatchling.build\"\nrequires = [\"hatchling\"]\n\n[tool.pixi.workspace]\nchannels = [\"conda-forge\"]\nplatforms = [\"win-64\"]\n\n[tool.pixi.pypi-dependencies]\nfastapi_app = { path = \".\", editable = true }\n\n[tool.pixi.tasks]\n\n[tool.pixi.dependencies]\npython = \"3.12.*\"\n\nPinning strategy\nThe thing that bothered me the most when I started with pixi is that the pinning of the packages. By default, pixi will use a very strict pinning strategy as you can see with fastapi for example: \"fastapi&gt;=0.115.14,&lt;0.116\", even if the user didn’t specify a version when adding fastapi.\nIf later you’d like to install a package that is not compatible with the pinned version of fastapi (even though you don’t care about the specific minor version of fastapi shown in the pyproject.toml, or the upper bound constraint), you’ll get an error, and this was frustrating.\npixi developers explain why they chose this strategy and discuss the matter at length in this GitHub issue.\nNonetheless, you can override the pinning strategy by using the pinning-strategy configuration but we’ll look at pixi’s config file later.\n\n\n\nManaging environments with features\nOne of pixi’s amazing features is being able to manage different sets of dependencies for different purposes (like the example for the run, build, test, etc above) using features. A feature (also called a dependency group is just a named set of dependencies).\nBy default, when adding packages, pixi will automatically add packages to the standard group of dependencies. You can add packages to a specific feature by using the --feature flag.\nLet’s say that our core dependencies that are needed for running the app are fastapi and uvicorn. Let us add two families of dependencies (two features): - A test feature that will include pytest & pytest-cov\npixi add --feature test pytest pytest-cov\n\nA dev feature that will include packages needed for development like ruff\n\npixi add --feature dev ruff\nWhen you’ll add this second feature, you’ll get a warning saying that the test feature was added but is not used by any environment and that is ok as we’re going to do it just after.\nNow, if you look at the pyproject.toml file, you’ll see that the dependencies are now grouped by features:\n[project]\nauthors = [{name = \"Safouane Chergui\", email = \"chsafouane@gmail.com\"}]\nname = \"fastapi_app\"\nrequires-python = \"&gt;= 3.11\"\nversion = \"0.1.0\"\ndependencies = [ \"fastapi&gt;=0.115.14,&lt;0.116\", \"uvicorn[standard]&gt;=0.35.0,&lt;0.36\"]\n\n[build-system]\nbuild-backend = \"hatchling.build\"\nrequires = [\"hatchling\"]\n\n[tool.pixi.workspace]\nchannels = [\"conda-forge\"]\nplatforms = [\"win-64\"]\n\n[tool.pixi.pypi-dependencies]\nfastapi_app = { path = \".\", editable = true }\n\n[tool.pixi.tasks]\n\n[tool.pixi.dependencies]\npython = \"3.12.*\"\n\n[tool.pixi.feature.test.dependencies]\npytest = \"*\"\npytest-cov = \"*\"\n\n[tool.pixi.feature.dev.dependencies]\nruff = \"*\"\n\nCreating environments from dependency groups (features)\nIn pixi, every environment is a collection of features (can be two features or more). The main project dependencies added without any feature like fastapi and uvicorn are added to an implicit default feature and to a default environment. If you execute\n pixi project environment list\nYou’ll see that the default environment is called default and it includes the default feature.\nEnvironments:\n- default:\n    features: default\nWhen you create a feature like test, pixi will create an environment from the default feature + the test feature, unless you explicitly say that you don’t want to do so. This means, that by default, the test environment isn’t composed of just the dependencies in the test feature but also the dependencies in the default feature: - All dependencies from the default feature (fastapi, uvicorn) - All dependencies from the test feature (pytest, pytest-cov)\nBefore creating the environments, let us tackle one last thing: the solve-groups.\nImagine having the default environment that includes fastapi and uvicorn and a test environment that includes additionally pytest and pytest-cov. When pixi will resolve the dependencies, the default environment can have different versions of fastapi and uvicorn than the test environment. To force pixi to group both environments together at the solve stage, you need to say that the test environment should be solved together with the default environment by using the --solve-groups flag.\nHere’s the documentation definition of the --solve-groups flag:\n\nsolve-group: String: The solve group is used to group environments together at the solve stage. This is useful for environments that need to have the same dependencies but might extend them with additional dependencies. For instance when testing a production environment with additional test dependencies.\n\nLet us create the environments now:\nTest environment:\nWe’re saying that we want to create a test_env environment that includes the test feature and that we want to solve it together with the default environment (the one that includes fastapi and uvicorn).\npixi project environment add fastapi-test-env --feature test --solve-group default\nDev environment:\nWe’re saying that we want to create a test_env environment that includes the test feature and that we want to solve it together with the default environment (the one that includes fastapi and uvicorn).\npixi project environment add fastapi-dev-env --feature test --feature dev --solve-group default\nNow, if you list the environments, you’ll see that the test_env and dev_env are created and that they include the features we specified:\npixi project environment list\n\n\nEnvironments:\n- default:\n    features: default\n- fastapi-test-env:\n    features: test, default\n    solve_group: default\n- fastapi-dev-env:\n    features: test, dev, default\n    solve_group: default\nIf you look at the pyproject.toml file, you’ll see that you have a new section called [tool.pixi.environments] that includes the environments you created:\n[tool.pixi.environments]\nfastapi-test-env = { features = [\"test\"], solve-group = \"default\" }\nfastapi-dev-env = { features = [\"test\", \"dev\"], solve-group = \"default\" }\nAll of this can be added manually instead to the pyproject.toml but it’s error prone and the pixi CLI is honestly very handy.\n\n\nEnvironments installation\nNow, let us create the environments by first install the default environment\npixi install\nYou can also simply run pixi shell to install the default environment and open a shell in it.\nTo install the dev environment, you can run:\npixi install fastapi-dev-env\nYou can also install all the environments at once using the flag --all to install:\npixi install --all\nNow you can any one of the environment inside the shell by running for example:\npixi shell fastapi-dev-env"
  },
  {
    "objectID": "posts/Pixi intro/Why I ditched conda & pip for Pixi.html#creating-tasks",
    "href": "posts/Pixi intro/Why I ditched conda & pip for Pixi.html#creating-tasks",
    "title": "Why I ditched pip & conda for Pixi",
    "section": "Creating tasks",
    "text": "Creating tasks\nTo create a task, you can use the pixi task add command and you’ll have to specify two things:\n\nThe task name\nThe command to run\n\nExecute pixi task add --help to see the available options, as you can add for example environment variables or isolate the task from the shell when running (not having access to the shell variables for example) among other things.\nLet us create a task to start a uvicorn server with hot reloading. The task will have as a name start. The command will add the task to the pyproject.toml file under the [tool.pixi.tasks] section.\npixi task add start \"uvicorn my_app.main:app --reload --host 0.0.0.0\"\nLet us also add a linting task that uses ruff\npixi task add lint \"ruff check src --fix\"\nIf you look now at the pyproject.toml file, you’ll see that the tasks are added under the [tool.pixi.tasks] section:\n[tool.pixi.tasks]\nstart = \"uvicorn my_app.main:app --reload --host 0.0.0.0\"\nlint = { task = \"ruff check src --fix\", environment = \"fastapi-dev-env\" }\n\nRunning tasks\nTo run the linting task in the dev environment, you can run:\npixi run -e fastapi-dev-env lint\nYou’ll get the following output:\nPixi task (lint in fastapi-dev-env): ruff check src --fix\nAll checks passed!\nNow, you can specify in the pyproject.toml the default environment in which the task should run but I haven’t found a way to do it through the CLI yet.\n[tool.pixi.tasks]\nstart = \"uvicorn my_app.main:app --reload --host 0.0.0.0\"\nlint = { task = \"ruff check src --fix\", environment = \"fastapi-dev-env\" }\nAs I can’t go through everything you can do with tasks, I’ll just list the things that I find useful but you can find more in the pixi documentation:\n\nYou can create a task that is composed of many tasks using the dependes-on field. that for example executes the linting task and then runs the app\nYou can create a tasks that runs the same task in multiple environments. If for example you’d like to test your code against multiple python versions, you can create a task that runs the same task in environments with different python versions (instead of using matrices of environments in CI/CD). Here an example from pixi’s documentation:\n\n# Task that depends on other tasks in different environments\n[tasks.test-all]\ndepends-on = [\n  { task = \"test\", environment = \"py311\" },\n  { task = \"test\", environment = \"py312\" },\n]\n\nYou can add environment variables or isolate the task when running from the shell (and thus not having access to the shell variables).\nIf a task depends on another task, you can cache the result of the first task and use it in the second task. Pixi won’t rerun the first task after doing some verifications that can be found in the documentation."
  },
  {
    "objectID": "posts/Pixi intro/Why I ditched conda & pip for Pixi.html#why-another-config-file",
    "href": "posts/Pixi intro/Why I ditched conda & pip for Pixi.html#why-another-config-file",
    "title": "Why I ditched pip & conda for Pixi",
    "section": "Why another config file ?",
    "text": "Why another config file ?\nThe pyproject.toml (or the pixi.toml) file reprensents the configuration of the pixi project. It includes the project metadata, the dependencies, the environments, the tasks, etc.\nThere is additional configuration that is not required for the project per say but in a way changes the behavior you would place in a config.toml file.\nYou can set this config at one of three levels: - locally: in this case, the configuration will be stored your_project/.pixi/config.toml and will impact only the current project. - globally: in this case, the configuration will be stored in $PIXI_HOME/config.toml and will impact all the projects using pixi. - system-wide: in this case, the configuration will be stored in /etc/pixi/config.toml and will impact all the projects using pixi.\nYou can also use the pixi config set &lt;some_config_key&gt; &lt;some_config_value&gt; command to set the configuration. While I will show you right away the keys that I find useful, you can find the full list of configuration keys as of version 0.49 that you can set:\n  │ Supported keys:\n  │     default-channels,\n  │     authentication-override-file,\n  │     tls-no-verify,\n  │     mirrors,\n  │     detached-environments,\n  │     pinning-strategy,\n  │     max-concurrent-solves,\n  │     repodata-config,\n  │     repodata-config.disable-jlap,\n  │     repodata-config.disable-bzip2,\n  │     repodata-config.disable-zstd,\n  │     repodata-config.disable-sharded,\n  │     pypi-config,\n  │     pypi-config.index-url,\n  │     pypi-config.extra-index-urls,\n  │     pypi-config.keyring-provider,\n  │     shell,\n  │     shell.force-activate,\n  │     shell.source-completion-scripts,\n  │     shell.change-ps1,\n  │     s3-options,\n  │     s3-options.&lt;bucket&gt;,\n  │     s3-options.&lt;bucket&gt;.endpoint-url,\n  │     s3-options.&lt;bucket&gt;.region,\n  │     s3-options.&lt;bucket&gt;.force-path-style,\n  │     experimental.use-environment-activation-cache,\n  │     proxy-config,\n  │     proxy-config.https,\n  │     proxy-config.http,\n  │     proxy-config.non-proxy-hosts"
  },
  {
    "objectID": "posts/Pixi intro/Why I ditched conda & pip for Pixi.html#useful-keys",
    "href": "posts/Pixi intro/Why I ditched conda & pip for Pixi.html#useful-keys",
    "title": "Why I ditched pip & conda for Pixi",
    "section": "Useful keys",
    "text": "Useful keys\n\nUsing private conda & PyPI repositories\nSome of my very security-oriented customers usually have their own conda and pip repositories (like nexus) and oblige everyone to use them as they only include packages that are approved by the security team.\nFor this, I use pypi-config.index-url and pypi-config.extra-index-urls to specify the index URL and the extra index URLs to use for PyPI packages.\npixi config set pypi-config.index-url https://nexus.some_random_company.com/pypi/simple\nLooking at the documentation, these can also be added to the pyproject.toml file under the [tool.pixi.pypi-options] section but I’ve never added them here.\n[tool.pixi.pypi-options]\n# Public packages will be sourced from the official PyPI\nindex-url = \"https://nexus.some_random_company.com/pypi/simple\"\n# Internal packages will be searched for here first\nextra-index-urls = [\"https://nexus.some_additional_random_company.com/pypi/simple\"]\nFor conda, I add the channels to the channels entry under the [tool.pixi.workspace] section in the pyproject.toml file:\n[tool.pixi.workspace]\nchannels = [\n    \"https://nexus.some_random_company.com/conda-forge\", \n    \"https://nexus.some_random_company_second.com/conda-forge\"\n]\nplatforms = [\"win-64\"]\nIf you need to manage credentials for private repositories, you can check pixi auth login.\n\n\nPinning strategy\nThe other key that I find useful is the pinning-strategy key. As I said before, by default, pixi uses a very strict pinning strategy that can be annoying at times. You can change it to one of the strategies listed in the documentation.\nPersonally, I like to pin to the major version using:\npixi config set pinning-strategy major\nThis might not be a very good practice as you can see here but it works just fine for my needs."
  },
  {
    "objectID": "posts/PDF parsing is hard/Parsing PDFs is hard.html",
    "href": "posts/PDF parsing is hard/Parsing PDFs is hard.html",
    "title": "PDF parsing is hard",
    "section": "",
    "text": "The goal of this blogpost is to explain what a PDF is internally and why parsing PDF files is not that easy.\nLately, I’ve been working on a project with a customer where the goal is to extract some specific information from PDF documents. To my big surprise, this task has proven to be quite challenging."
  },
  {
    "objectID": "posts/PDF parsing is hard/Parsing PDFs is hard.html#what-the-hell-is-a-pdf-document",
    "href": "posts/PDF parsing is hard/Parsing PDFs is hard.html#what-the-hell-is-a-pdf-document",
    "title": "PDF parsing is hard",
    "section": "What the hell is a PDF document ?",
    "text": "What the hell is a PDF document ?\n\nA first glimpse at a PDF file internal structure\nHave you ever opened a PDF document with notepad or vscode instead of your preferred PDF reader ? If you do so , you’ll stumble upon something that looks like this :\n\n\n\nFigure 1: Internal structure of a PDF document when viewed as raw text\n\n\nIf you’d like to see the full PDF internal structure, you can find the example PDF here.\n\n\nPDF Page Description Language\nTo understand why PDFs are hard to parse, one must understand how a PDF file is built.\nA PDF file is based on Page Description Language (PDL), which is a language used to describe the layout and appearance of a printed page. PDF PDL provides a standardized set of commands to reconstruct a page with perfect fidelity.\nAs a result, a PDF file is essentially a collection of instructions for rendering a page, rather than a linear sequence of text and images. If you look at the example pdf available in the github gist, you’ll see starting line 34 the following commands:\n/F1 18 Tf\n100 700 Td\n(This is a PDF tutorial) Tj\nWhat the following instructions do is: - /F1 18 Tf : set the font to F1 with size 18 - 100 700 Td : move the text position to (100, 700) - (This is a PDF tutorial) Tj : show the text string\nEvery PDF looks just like this; a precise sequence of commands that specify what to draw and exactly at what coordinates. It does not contain a semantic representation of its content. It does not state, “This is a paragraph that flows through two columns” or “this is a table”.\nA table, for example, is just a grid of lines and text positioned at specific coordinates. There are no inherent relationships between the cells, no indication of headers or footers, and no understanding of the data contained within.\nSo when a parser sees what’s supposed to be a table, it sees just a bunch of lines and text. Its task (rather difficult task) is to infer the structure and relationships between these elements.\nThis lack of semantic structure makes it challenging to parse complex PDF documents.\n\n\nThe internal structure of a PDF\nWhat you see in Figure 1 or in the gist file is the internal structure of a PDF document. Let us dive into the key components that make up this structure.\nA PDF is composed internally of four sections:\n\n\n\nFigure 2: Internal structure of a PDF document\n\n\nSource: ResearchGate - An example of the PDF file structure\n\n\n\nThe header\nThe header of a PDF file tells you about the PDF specifications version used to generate it. It is always the first line of the file and starts with the %PDF- marker. In Figure 1, it corresponds to %PDF-1.7.\n\n\nThe body\nNow, the body is where you “define” the content of the PDF."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Weekends Eureka",
    "section": "",
    "text": "PDF parsing is hard\n\n\n\nPython\n\nPDF\n\n\n\n\n\n\n\n\n\nAug 27, 2025\n\n\nSafouane Chergui\n\n\n\n\n\n\n\n\n\n\n\n\nWhy I ditched pip & conda for Pixi\n\n\n\nPython\n\nPackage management\n\n\n\n\n\n\n\n\n\nJul 5, 2025\n\n\nSafouane Chergui\n\n\n\n\n\n\n\n\n\n\n\n\nByte Pair Encoding Tokenization\n\n\n\nPython\n\nNLP\n\n\n\n\n\n\n\n\n\nJul 7, 2024\n\n\nSafouane Chergui\n\n\n\n\n\nNo matching items"
  }
]