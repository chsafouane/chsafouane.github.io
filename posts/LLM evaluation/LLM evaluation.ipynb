{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "934daf49",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "title: \"Stop Vibe-Checking: Real-World Lessons on LLM Evals\"\n",
    "author: \"Safouane Chergui\"\n",
    "date: \"2025-11-19\"\n",
    "format: html\n",
    "toc: true\n",
    "toc-location: body\n",
    "toc-depth: 4\n",
    "categories: [LLM, Agent, Evaluation]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a9b69",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Deploying LLM-powered systems in production is the easy part. The hard part? Making sure they're actually working.\n",
    "\n",
    "I've been deploying LLM-powered systems in production in many companies and across different industries for almost 3 years now. Each time, I encountered the same critical challenge: **how do you truly evaluate whether your LLM is performing well and not only rely on vibe checks ?**\n",
    "\n",
    "If you're struggling to move beyond \"it looks good to me\" when evaluating your LLM applications, this blog post is for you.\n",
    "\n",
    "<br><br>\n",
    "<div align=\"center\">\n",
    "<img src=\"./assets/front_image.jpg\" width=\"80%\" style=\"display: block; margin: 0 auto;\">\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72389db",
   "metadata": {},
   "source": [
    "This blog post contains lessons learnt through hands-on experience. These lessons come mostly from my experience testing what I have learnt in deploying real-life LLM applications, talking with peers, doing courses, and reading blog posts.\n",
    "\n",
    "Apart from my personal experience, [Hamel Husain](https://www.linkedin.com/in/hamelhusain/) & [Shreya Shankar](https://www.linkedin.com/in/shrshnk/) both [course](https://maven.com/parlance-labs/evals) & blogs on LLM evaluation have been of a great help to me and many of the techniques I discuss here are either directly quoted from their work or highly inspired by it.\n",
    "\n",
    "Each of the lessons below will tackle a specific part of building and evaluating real LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac788d6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Pre-lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ea9df6",
   "metadata": {},
   "source": [
    "I can't emphasize this enough and even though you've probably heard it a million times before, I'll have to say it: **KNOW YOUR PRODUCT AND YOUR USERS**.\n",
    "\n",
    "If you don't know your users and your product well, you won't be able to understand the different ways they'll interact with your system, the different types of queries they'll make, and the different ways they can express the same intent. This is going to be a major obstacle whether you want to create synthetic data, to create evaluation metrics, or to interpret the results of your evaluations.\n",
    "\n",
    "Now that this is said, let's dive into the lessons!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807f565",
   "metadata": {},
   "source": [
    "# Lesson 1: I'll vibe-check my app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2b13f9",
   "metadata": {},
   "source": [
    "Here's a common scenario I've encountered many times: a company builds a RAG system over their product documentation, and naturally wants to evaluate and improve it. But there's a pattern I keep seeing so often, companies expecting AI to \"just work\" out of the box.\n",
    "\n",
    "This expectation shows up most clearly in evaluation practices. Companies often use \"vibe checks\" by manually asking a dozen of questions and eyeballing whether the system answers seem reasonable.\n",
    "\n",
    "This is a terrible way to evaluate your LLM applications for multiple reasons:\n",
    "\n",
    "- The queries you're going to ask are biased towards what you think the system should be able to do or towards a specific range of queries that you expect the final user to enter. You will likely miss most of the cases and failure modes that you didn't think about.\n",
    "From first-hand experience, the users usually will surprise you with the way they write their queries and the types of queries they will enter. You should expect them to write them as if they are in a rush ðŸ˜…\n",
    "\n",
    "- Not having evaluated your app in a systematic and/or scalable way will lead to have to deal with \"whack-a-mole\" situation (as Hamel so beautifully puts it) where you fix one by changing some prompt only to have another one pop up somewhere else.\n",
    "This will lead to frustration from your stakeholders, frustration from the team working on the app, and maybe even to a a lack of trust in LLMs whithin your organization.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExdm55dDYxNHd5eHdmYzlnZmJnMTg4OHRvb2x4ZWQ4bHVnZzQ2am9waiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/ebITvSXYKNvRm/giphy.gif\" width=\"20%\" style=\"display: block; margin: 0 auto;\">\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb4d3be",
   "metadata": {},
   "source": [
    "# Lesson 2: I don't have any data to test my application, where do I start ?\n",
    "\n",
    "Once you develop your LLM-powered app, you are faced with **\"Which comes first, the chicken or the egg?\"** dilemma:\n",
    "\n",
    "- You don't have real user queries data because you haven't deployed you app yet\n",
    "- You can't deploy your app yet because you haven't tested it with real user queries yet\n",
    "\n",
    "## Sub-lesson 1: Real data is better than synthetic data\n",
    "\n",
    "You can almost always get some pseudo-real data. If you can't have access to some beta users, ask your teammates to test the system. They will have very probably some biases of their own, but at least you will get some data that is not completely synthetic and that has different characteristics because it's coming from different people.\n",
    "\n",
    "## Sub-lesson 2: The bad way to create synthetic data\n",
    "Real data is always better than synthetic data. But hey, if you really can't have some real data, then synthetic data is the way to go.\n",
    "\n",
    "The mistake most people do when creating synthetic data is to ask an LLM to generate queries that are similar to what they expect the users to enter. This is a also a bad idea as the generated queries will be biased towards what you think the users will enter and will likely miss many failure modes.\n",
    "Most importantly, the generated queries will likely be \"too good\" and not representative of real user queries (messy, mispellings, incomplete...).\n",
    "\n",
    "I've trained a retriever in the past on synthetic data generated this way. While the performance on synthetic-queries-like was really good, the performance on real user queries was really bad. The gap between synthetic data and real user data was just too big.\n",
    "\n",
    "## Sub-lesson 3: The good way to create synthetic data\n",
    "\n",
    "- *Think of dimensions of variability of user queries:*\n",
    "An approach that I have learnt from Hamel Husain & Shreya Shankar is to first think about the different dimensions of variability in user queries for your specific application.\n",
    "For example, if you're building a RAG over a technical product documentation, you can think about several dimensions of variability, such as:\n",
    "\n",
    "- User type (user, admin, developer, etc.)\n",
    "- Intent (seeking information, troubleshooting, feature requests, etc.)\n",
    "- User expertise level (beginner, intermediate, expert, etc.)\n",
    "- Query length (short queries, long queries, etc.)\n",
    "- Query complexity (simple queries, complex queries with multiple sub-questions, etc.)\n",
    "- Query style (formal, informal, typos, etc.)\n",
    "- etc.\n",
    "\n",
    "As a query always depends on the context of the application and the persona of the users (a wink ðŸ˜‰ to the pre-lesson above), a dimension should be really specific to your application and not some general dimensions that someone else has used in another context.\n",
    "\n",
    "Then, for each dimension, you can brainstorm different values that the dimension can take (or delegate the task of brainstorming values of some dimensions to an LLM). For example, for the \"user expertise level\" dimension, you can have the values: \"beginner\", \"intermediate\", \"expert\" as shown above.\n",
    "\n",
    "Once the list of dimensions and their possible values is ready, you can start combining them to create tuples that will represent different synthetic queries.\n",
    "\n",
    "Here are some examples of tuples representing combinations of the dimensions mentioned above.\n",
    "\n",
    "```\n",
    "(\"end_user\", \"troubleshoot\", \"beginner\", \"short\", \"simple\", \"typos\")\n",
    "(\"developer\", \"integration_info\", \"expert\", \"long\", \"complex\", \"formal\")\n",
    "(\"admin\", \"permissions_help\", \"intermediate\", \"short\", \"simple\", \"informal\")\n",
    "(\"end_user\", \"feature_discovery\", \"beginner\", \"short\", \"simple\", \"incomplete\")\n",
    "(\"support_engineer\", \"root_cause_analysis\", \"expert\", \"long\", \"complex\", \"dense\")\n",
    "(\"end_user\", \"account_status\", \"beginner\", \"short\", \"simple\", \"mixed_case\")\n",
    "(\"developer\", \"performance_optimization\", \"expert\", \"medium\", \"complex\", \"typos\")\n",
    "(\"admin\", \"audit_logging\", \"intermediate\", \"medium\", \"moderate\", \"formal\")\n",
    "(\"end_user\", \"error_meaning\", \"beginner\", \"short\", \"simple\", \"abbreviations\")\n",
    "```\n",
    "\n",
    "Each tuple becomes a prompt seed you can use to generate multiple queries from ðŸš€\n",
    "\n",
    "And here you have it, asystematic way to create synthetic data that covers a wide range of possible user queries for your specific application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8751f53c",
   "metadata": {},
   "source": [
    "# Lesson 3: Have a systematic way to identify failure modes\n",
    "\n",
    "When you start evaluating your LLM applications, you need to have a systematic way to identify failure modes. Not having a systematic way to identify failure modes will lead you to the same \"whack-a-mole\" situation mentioned earlier where you fix one failure mode only to have another one pop up somewhere else.\n",
    "\n",
    "You can identify failure modes using synthetic data (created as explained in Lesson 2) or real user queries (if you have access to them) even before deploying your app in production.\n",
    "\n",
    "## Sub-lesson 1: The three steps to identify failure modes\n",
    "\n",
    "**Traces are king:**\n",
    "Now that you have a set of user queries (either real or synthetic), input each query through your LLM application and generate the whole trace of the execution. The trace should include all the intermediate steps, LLM calls, tool calls, and final output.\n",
    "\n",
    "This trace will be your best friend when it comes to identifying failure modes. By looking at the trace, you can see exactly where things went wrong.\n",
    "\n",
    "**Axial coding:**\n",
    "\n",
    "Once you go through a good number of traces \n",
    "\n",
    "\n",
    "\n",
    "## Sub-lesson 2: Clustering production queries didn't work for me\n",
    "\n",
    "One approach that I've tested in the past (amounts to 1 year) and that didn't work that well for me is to identify a set of classes that I expect user queries in my app to fall into. Identifying the classes helps in knowing which queries the model is not handling well and which classes of queries need more attention.\n",
    "\n",
    "On top of the identified classes, I would create a category for \"other\" queries that don't fall into any of the identified classes.\n",
    "an analysis of the queries that have fallen into the \"other\" category helped me identify new classes that I hadn't thought about initially and better pinpoint the analysis of the performance of the model on different types of queries.\n",
    "\n",
    "At the time, I used GPT-4 to do the classification. What didn't work well for me was most queries ended up in the \"other\" category, defeating the purpose of the classification. This pattern showed up with a couple of other decoder LLMs.\n",
    "\n",
    "While I thought about finetuning a Bert-like model to do the classification, having to finetune a new model whenever a new class is identified seemed like an overkill and a maintenance nightmare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36effd7b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```{dot}\n",
    "//| fig-width: 6\n",
    "//| fig-height: 8\n",
    "//| fig-align: center\n",
    "digraph G {\n",
    "    rankdir=TB;\n",
    "    node [fontname=\"Arial\", fontsize=11];\n",
    "    edge [fontname=\"Arial\", fontsize=10];\n",
    "    \n",
    "    // Nodes\n",
    "    Start [label=\"Define Query Classes\\n+ 'Other' Category\", shape=box, style=\"rounded,filled\", fillcolor=\"#e6f3ff\", color=\"#0066cc\", penwidth=2];\n",
    "    Classify [label=\"GPT-4\\nClassification\", shape=diamond, style=filled, fillcolor=\"#fff9e6\", width=1.8];\n",
    "    ToClass [label=\"Few Queries:\\nDefined Classes\", shape=box, style=rounded];\n",
    "    ToOther [label=\"Most Queries:\\n'Other' Category\", shape=box, style=\"rounded,filled\", fillcolor=\"#ffe6e6\", color=\"#ff6666\", penwidth=2];\n",
    "    Problem [label=\"âš ï¸ PROBLEM:\\nMost queries in 'Other'\\nDefeats Purpose!\", shape=box, style=\"filled\", fillcolor=\"#ffcccc\", color=\"#ff0000\", penwidth=3];\n",
    "    ConsiderBERT [label=\"Try BERT\\nFinetuning?\", shape=box, style=rounded];\n",
    "    Problem2 [label=\"âš ï¸ PROBLEM:\\nMust retrain for\\neach new class\\nMaintenance Nightmare!\", shape=box, style=\"filled\", fillcolor=\"#ffcccc\", color=\"#ff0000\", penwidth=3];\n",
    "    \n",
    "    // Edges\n",
    "    Start -> Classify;\n",
    "    Classify -> ToClass [label=\"Few\", fontsize=9];\n",
    "    Classify -> ToOther [label=\"Most\", fontsize=9, penwidth=2];\n",
    "    ToClass -> Problem;\n",
    "    ToOther -> Problem [penwidth=2];\n",
    "    Problem -> ConsiderBERT;\n",
    "    ConsiderBERT -> Problem2;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b0a0fe",
   "metadata": {},
   "source": [
    "# Lesson 4: Off-the-shelf evals don't work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139deaae",
   "metadata": {},
   "source": [
    "# Lesson 5: The evaluation interface matters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838e82a",
   "metadata": {},
   "source": [
    "When using an interface to evaluate your LLM applications, the interface matters a lot. A good interface should in general empower you to spot the failure modes and annotate your data easily.\n",
    "\n",
    "I've worked with many vendors providing LLM evaluation interfaces for LLM applications. Their interfaces are quite similar and are actually pretty good to hit the ground running. They will allow you to go through the traces of your LLM applications, see the inputs and outputs, and annotate them with a few clicks. \n",
    "However, what I usually find lacking is the following:\n",
    "\n",
    "- One pattern that slows me most is not having a specific field for failure mode annotation. The slowing down appears most when for example some failure modes have already appeared in the data but I'll have to write them down again and again for each new trace that I see. Having a specific field for failure mode annotation or just the most recurrent failure modes as options to select from would speed up the annotation process a lot.\n",
    "\n",
    "- Formatting ðŸ’„ is not customizable: Imagine you have an app for writing emails. When doing your error analysis, you want to see the email formatted as it would appear in an email client (with subject, greeting, body, signature, etc.). This will allow you to easily spot the failure modes instead of having to look at a JSON or a plain blob of text (which is the case of all the interfaces I know).\n",
    "The absence of appropriate formatting slows down the error analysis process a lot.\n",
    "If you want to know more about this topic, I recommend you read [this talk](https://chrislovejoy.me/llm-native-expert-system) I wrote by Christopher Lovejoy on building LLM-native apps in vertical industries."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
