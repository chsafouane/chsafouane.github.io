{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "934daf49",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "title: \"Stop Vibe-Checking: 5 Battle-Tested Lessons for LLM Evals\"\n",
    "author: \"Safouane Chergui\"\n",
    "date: \"2025-11-19\"\n",
    "format: html\n",
    "toc: true\n",
    "toc-location: body\n",
    "toc-depth: 4\n",
    "categories: [LLM, Evaluation]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a9b69",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Deploying LLM-powered systems in production is the easy part. The hard part? Making sure they're actually working.\n",
    "\n",
    "I've been deploying LLM-powered systems in production in many companies and across different industries for almost 3 years now. Each time, I encountered the same critical challenge: **how do you truly evaluate whether your LLM is performing well and not only rely on vibe checks ?**\n",
    "\n",
    "If you're struggling to move beyond \"it looks good to me\" when evaluating your LLM applications, this blog post is for you.\n",
    "\n",
    "<br><br>\n",
    "<div align=\"center\">\n",
    "<img src=\"./assets/front_image.jpg\" width=\"80%\" style=\"display: block; margin: 0 auto;\">\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72389db",
   "metadata": {},
   "source": [
    "This blog post contains lessons learnt through hands-on experience. These lessons come mostly from my experience testing what I have learnt in deploying real-life LLM applications, talking with peers, doing courses, and reading blog posts.\n",
    "\n",
    "Apart from my personal experience, [Hamel Husain](https://www.linkedin.com/in/hamelhusain/) & [Shreya Shankar](https://www.linkedin.com/in/shrshnk/) both [course](https://maven.com/parlance-labs/evals) & blogs on LLM evaluation have been of a great help to me and many of the techniques I discuss here are inspired by their work.\n",
    "\n",
    "Each of the lessons below will tackle a specific part of building and evaluating real LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807f565",
   "metadata": {},
   "source": [
    "# Lesson 1: I'll vibe-check my app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2b13f9",
   "metadata": {},
   "source": [
    "Here's a common scenario I've encountered many times: a company builds a RAG system over their product documentation, and naturally wants to evaluate and improve it. But there's a pattern I keep seeing so often, companies expecting AI to \"just work\" out of the box.\n",
    "\n",
    "This expectation shows up most clearly in evaluation practices. Companies often use \"vibe checks\" by manually asking a dozen of questions and eyeballing whether the system answers seem reasonable.\n",
    "\n",
    "This is a terrible way to evaluate your LLM applications for multiple reasons:\n",
    "- The queries you're going to ask are biased towards what you think the system should be able to do or towards a specific range of queries that you expect the final user to enter. You will likely miss most of the cases and failure modes that you didn't think about.\n",
    "From first-hand experience, the users usually will surprise you with the way they write their queries and the types of queries they will enter. You should expect them to write them as if they are in a rush ðŸ˜…\n",
    "\n",
    "- Not having evaluating your app in a systematic and/or scalable way will lead to have to deal with \"whack-a-mole\" situation (as Hamel so beautifully puts it) where you fix one by changing some prompt only to have another one pop up somewhere else.\n",
    "This will lead to frustration from your stakeholders, frustration from the team working on the app, and maybe even to a a lack of trust in LLMs whithin your organization.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExdm55dDYxNHd5eHdmYzlnZmJnMTg4OHRvb2x4ZWQ4bHVnZzQ2am9waiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/ebITvSXYKNvRm/giphy.gif\" width=\"20%\" style=\"display: block; margin: 0 auto;\">\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb4d3be",
   "metadata": {},
   "source": [
    "# Lesson 2: I don't have any data to test my application, where do I start ?\n",
    "\n",
    "**Sub-lesson 1: Real data is better than synthetic data**\n",
    "\n",
    "**Sub-lesson 2: The bad way to create synthetic data**\n",
    "\n",
    "**Sub-lesson 3: The good way to create synthetic data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8751f53c",
   "metadata": {},
   "source": [
    "# Lesson 3: Have a systematic way to identify failure modes\n",
    "\n",
    "When you start evaluating your LLM applications, you need to have a systematic way to identify failure modes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b0a0fe",
   "metadata": {},
   "source": [
    "# Lesson 4: Off-the-shelf evals don't work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139deaae",
   "metadata": {},
   "source": [
    "# Lesson 5: The evaluation interface matters"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
