{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c0fbb2f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"All you need to know about RoPE\"\n",
    "author: \"Safouane Chergui\"\n",
    "date: \"2026-01-08\"\n",
    "categories: [Python, NLP]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ae34a",
   "metadata": {},
   "source": [
    "I've discovered RoPE through tutorials and I wanted to read the paper for quite a while. I've finally gotten to it.\n",
    "Even though I understood the general idea and how it works, some questions kept buzzing in my mind.\n",
    "So, in this blog post, I'll try to start from 0 and take you to a full understanding of RoPE.\n",
    "\n",
    "This post will have a question-answer format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ec19b5",
   "metadata": {},
   "source": [
    "# Q1: Why do we need position encoding at all ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eedb56",
   "metadata": {},
   "source": [
    "Consider these two sentences:\n",
    "\n",
    "- Sentence 1: `The cat chased the mouse`\n",
    "- Sentence 2: `The mouse chased the cat`\n",
    "\n",
    "These sentences have a different meaning but they are the same for vanilla attention. This is because attention is **permutation invariant**.\n",
    "\n",
    "To understand this property, let us look at the attention formula:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5171d8b2",
   "metadata": {},
   "source": [
    "Let us see if the swapping of `cat` and `mouse` positions changes anything when it comes to the vanilla attention mechanism.\n",
    "\n",
    "In sentence 1, the attention score between `cat` at position 1 and `mouse` at position 4 is:\n",
    "\n",
    "$$\\text{score}_{1,4} = \\text{softmax}\\left(\\frac{q_{\\text{cat}} \\cdot k_{\\text{mouse}}}{\\sqrt{d_k}}\\right)$$\n",
    "\n",
    "In sentence 2, the attention score between `mouse` at position 1 and `cat` at position 4 is:\n",
    "\n",
    "$$\\text{score}_{1,4} = \\text{softmax}\\left(\\frac{q_{\\text{mouse}} \\cdot k_{\\text{cat}}}{\\sqrt{d_k}}\\right)$$\n",
    "\n",
    "Since queries and keys are computed as $Q = W_Q \\cdot X$ and $K = W_K \\cdot X$ where $X$ is just the token embedding (no position info in X), the same token always produces the same query/key vector regardless of its position.\n",
    "\n",
    "Consequently, attention cannot distinguish who is chasing whom and the two sentences will have the exact attention matrix.\n",
    "\n",
    "The conclusion is that we must inject the position information into the attention mechanism. Otherwise, we're really just weighting a bag of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ec2e82",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
