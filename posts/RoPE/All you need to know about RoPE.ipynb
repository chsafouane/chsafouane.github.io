{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c0fbb2f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"All you need to know about RoPE\"\n",
    "author: \"Safouane Chergui\"\n",
    "date: \"2026-01-12\"\n",
    "categories: [Python, NLP]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ae34a",
   "metadata": {},
   "source": [
    "I've discovered RoPE through tutorials and I wanted to read the paper for quite a while. I've finally gotten to it.\n",
    "Even though I understood the general idea and how it works, some questions kept buzzing in my mind.\n",
    "So, in this blog post, I'll try to start from 0 and take you to a full understanding of RoPE.\n",
    "\n",
    "This post will have a question-answer format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ec19b5",
   "metadata": {},
   "source": [
    "# Q1: Why do we need position encoding at all ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eedb56",
   "metadata": {},
   "source": [
    "Consider these two sentences:\n",
    "\n",
    "- Sentence 1: `The cat chased the mouse`\n",
    "- Sentence 2: `The mouse chased the cat`\n",
    "\n",
    "These sentences have a different meaning but they are the same for vanilla attention. This is because attention is **permutation invariant**.\n",
    "\n",
    "To understand this property, let us look at the attention formula:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5171d8b2",
   "metadata": {},
   "source": [
    "Let us see if the swapping of `cat` and `mouse` positions changes anything when it comes to the vanilla attention mechanism.\n",
    "\n",
    "In sentence 1, the attention score between `cat` at position 1 and `mouse` at position 4 is:\n",
    "\n",
    "$$\\text{score}_{1,4} = \\text{softmax}\\left(\\frac{q_{\\text{cat}} \\cdot k_{\\text{mouse}}}{\\sqrt{d_k}}\\right)$$\n",
    "\n",
    "In sentence 2, the attention score between `mouse` at position 1 and `cat` at position 4 is:\n",
    "\n",
    "$$\\text{score}_{1,4} = \\text{softmax}\\left(\\frac{q_{\\text{mouse}} \\cdot k_{\\text{cat}}}{\\sqrt{d_k}}\\right)$$\n",
    "\n",
    "Since queries and keys are computed as $Q = W_Q \\cdot X$ and $K = W_K \\cdot X$ where $X$ is just the token embedding (no position info in X), the same token always produces the same query/key vector regardless of its position.\n",
    "\n",
    "Consequently, attention cannot distinguish who is chasing whom and the two sentences will have the exact attention matrix.\n",
    "\n",
    "The conclusion is that we must inject the position information into the attention mechanism. Otherwise, we're really just weighting a bag of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ec2e82",
   "metadata": {},
   "source": [
    "# Q2:\"Attention is All You Need\" solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1367d0",
   "metadata": {},
   "source": [
    "Now, the `Attention is All You Need\" paper came up with solution for encoding the position information into word embeddings.\n",
    "\n",
    "The idea is instead of just having as an input the embedding of the word `cat` (semantic embedding), they add a vector that encodes the position (an embedding for the position).\n",
    "\n",
    "$$\\text{input}_i = \\text{word\\_embedding}_i + \\text{position\\_embedding}_i$$\n",
    "\n",
    "So, in sentence 1 & sentence 2, the word `cat` would have two different representations because the position embedding differs between the two.\n",
    "\n",
    "Intuitively, they just add a vector that conveys information about the position to each embedding. The vector nudges the semantic vector in space a bit but not that much as to not put very far from its initial meaning in the space. You can gain more intuition about this from [Luis Serrano video on the topic](https://www.youtube.com/watch?v=IHu3QehUmrQ)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/transformer_architecture.png\" alt=\"Transformer Architecture\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068dbca1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
