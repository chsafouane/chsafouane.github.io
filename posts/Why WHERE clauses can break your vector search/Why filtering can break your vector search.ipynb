{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df36e36",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Why filtering breaks your vector search\"\n",
    "author: \"Safouane Chergui\"\n",
    "date: \"2026-02-06\"\n",
    "image: \"https://plus.unsplash.com/premium_photo-1681586126003-2a6d4ba943a2?q=80&w=1112&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\"\n",
    "format: html\n",
    "toc: true\n",
    "toc-location: body\n",
    "toc-depth: 4\n",
    "categories: [Python, NLP, vector search]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41613dd7",
   "metadata": {},
   "source": [
    "Semantic search works until you add a filter. You search for \"wireless earbuds\", get great results, but then you decide to add a filter on brands `WHERE brand IN ('Apple', 'Google')` and suddenly many results vanish or you get nothing back at all, even if the data has results for your query.\n",
    "\n",
    "This isn't a bug in your system but there is a reason why filtering breaks your vector search, and it all has to do with how HNSW works. \n",
    "\n",
    "This post explains the problem, quantifies how bad it gets, and walks through one of the elegant solutions there exists in vector DBs today which how Qdrant solves it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8bc327",
   "metadata": {},
   "source": [
    "# 1. A quick recap on HNSW\n",
    "\n",
    "Suppose you’re querying a vector DB with a query embedding `q`. An easy way to find the nearest neighbors is **exhaustive search**: compute the similarity between `q` and all the stored vectors, then take the top k results. That gives exact results but it scales linearly. With N items in your vector DB, you perform `O(N)` distance evaluations per query. This works great for small datasets but the latency grows rapidly once you start dealing with bigger datasets.\n",
    "\n",
    "A more realistic way to think about it: imagine you live in a world with no internet and you’re trying to find the best restaurant in an entire country. You wouldn’t inspect every street and every menu. Instead, you’d use a *navigation strategy*, maybe:\n",
    "\n",
    "- You'll start by choosing a city that is known for its good restaurants\n",
    "- Then you'll ask locals and head to likely part of town\n",
    "- Once in there, you'll ask locals again to go to the right neighborhood\n",
    "- Finally, you'll only compare restaurants in that neighborhood\n",
    "\n",
    "At the end of the day, you are trading a tiny risk of missing the absolute best restaurant in the country for a massive speedup.\n",
    "\n",
    "**Approximate Nearest Neighbor (ANN)** algorithms formalize this trade-off for vector search. In practice, many vector databases use **HNSW** (Hierarchical Navigable Small World) as their default ANN index. HNSW builds a graph that lets the query “walk” toward closer and closer candidates without scanning everything. \n",
    "\n",
    "Btw, if you need a good course about vector search, HNSW and all things related to this, take a look "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef677e",
   "metadata": {},
   "source": [
    "## How does it work?\n",
    "\n",
    "Think back to the “no internet” restaurant problem from the previous section. You want the best restaurant in an entire country. Doing it exactly would mean visiting every street and reading every menu. HNSW turns your \"ask the locals\" strategy into reality.\n",
    "\n",
    "### The intuition\n",
    "\n",
    "Your dataset is the country. Each item (vector) is a restaurant somewhere on the map. Your query vector is your personal taste.\n",
    "\n",
    "HNSW pre-builds “local recommendations”: each restaurant knows a handful of nearby restaurants (nearby in embedding space). At query time, you hop from one promising candidate to the next until you’re in the right neighborhood, then you refine the shortlist.\n",
    "\n",
    "### Under the hood\n",
    "\n",
    "HNSW stores a graph:\n",
    "\n",
    "- each vector is a node\n",
    "- each node connects to a small set of nearby nodes (neighbors), where *nearby* is defined by your similarity metric (often cosine)\n",
    "\n",
    "At query time, the search looks like this:\n",
    "\n",
    "1. Start from the index’s entry point (a specific node stored in the HNSW structure).\n",
    "2. Keep a *shortlist* of the best candidates you’ve seen so far.\n",
    "3. Repeatedly expand the most promising candidate by checking its neighbors and updating the shortlist.\n",
    "4. Stop when your budget is exhausted and return the top $K$ from the shortlist.\n",
    "\n",
    "The key knob at query time is `ef` (Qdrant: `hnsw_ef`) which determines **how wide that shortlist is allowed to be.** Bigger `ef` means you consider more alternatives (better recall) but do more distance checks (higher latency).\n",
    "\n",
    "Here's a table of the 3 main parameters for HNWS graph and their impact on the usability:\n",
    "\n",
    "| Parameter | When | What it controls | Bigger means… |\n",
    "|---|---|---|---|\n",
    "| `M` | build time | max neighbors per node | higher memory, often better connectivity/recall |\n",
    "| `ef_construction` | build time | how hard the builder tries to find good edges | slower build, higher index quality |\n",
    "| `ef` / `hnsw_ef` | query time | how wide the candidate shortlist is | higher latency, higher recall |\n",
    "\n",
    "### Why the hierarchy matters\n",
    "\n",
    "The strategy above works on a single graph, but it can waste time wandering before it reaches the right region.\n",
    "\n",
    "HNSW adds the *pick the right city first* step by using multiple layers for search.\n",
    "\n",
    "- The higher layer is like a very coarse-grained map of the country, including details just about very few restaurant that are very far from each other.\n",
    "- The lowest layer of all is like the map of the whole country with all the details. It includes all the nodes of all previous layer.\n",
    "\n",
    "In general terms:\n",
    "\n",
    "- upper layers contain fewer nodes with longer-range connections\n",
    "- lower layers contain more nodes with short-range connections\n",
    "\n",
    "When searching for the most similar nodes to your query, search is top-down.\n",
    "\n",
    "- You start in the top layer, walk greedily to the most similar node you can find until you can’t improve\n",
    "- You drop to the next layer while keeping your current best node as the new starting point\n",
    "- You repeat until the bottom layer (which contains all nodes)\n",
    "\n",
    "The hierarchy is what makes the navigation both fast and surprisingly accurate.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://qdrant.tech/courses/day2/hnsw-layers.png\" alt=\"HNSW greedy search visualization\" style=\"max-width: 100%; height: auto;\">\n",
    "</div>\n",
    "<p style=\"text-align: center; font-style: italic; color: #666; margin-top: 5px;\">\n",
    "Source: <a href=\"https://qdrant.tech/courses/day2/hnsw-layers.png\">Qdrant</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a2a821",
   "metadata": {},
   "source": [
    "# 2. How filtering breaks HNSW\n",
    "\n",
    "Vector search almost never happens in isolation as your users always want to combine similarity with metadata filters as in our earbuds example: *\"find wireless earbuds, but only from Apple or Google.\"*\n",
    "\n",
    "Two obvious ways to filter come to mind (which are implemented widely in industry) but they both fail most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa474430",
   "metadata": {},
   "source": [
    "## The post-filtering method\n",
    "\n",
    "The naive approach: run HNSW to get the top-K most similar results, then throw away the ones that don't match the filter.\n",
    "\n",
    "The main reason this method fails is that your top K *wireless earbuds* results might be dominated by other brands. After discarding non-matching items, you're **left with almost nothing**.\n",
    "\n",
    "HNSW returned 5 results but only 1 matched the filter. Meanwhile, closer Apple/Google earbuds ranked beyond top K were never retrieved, so they could not be returned.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"assets/post-filtering.png\" alt=\"Post-filtering method diagram\" style=\"max-width: 100%; height: auto;\">\n",
    "</div>\n",
    "\n",
    "In practice, you end up making `top k` higher (and usually the exploration budget `ef`) high enough that *some* filtered items make it into the candidate set, which defeats the whole purpose of ANN: you pay for extra work just to throw most of it away."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf6946",
   "metadata": {},
   "source": [
    "## The pre-filtering method\n",
    "\n",
    "The other option is to filter the dataset first and then search HNSW only among matching items.\n",
    "\n",
    "While this method looks sound, it also fails...\n",
    "\n",
    "The reason it fails is that the HNSW graph was built on **all data**. When you filter your dataset, you're removing nodes from the graph, so many edges disappear. Paths between matching items often route *through* non-matching ones, and once those bridge nodes are gone, the graph falls apart.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"assets/pre-filtering.png\" alt=\"Pre-filtering method diagram\" style=\"max-width: 100%; height: auto;\">\n",
    "</div>\n",
    "\n",
    "The path from Apple Earbuds to Google Earbuds went *through* Sony Earbuds. Remove non-matching brands, and Google Earbuds becomes **unreachable**. Your search silently misses it.\n",
    "\n",
    "Even worse, if the HNSW entry point (or the early candidates in the queue) happen to be in the filtered-out region, the search may never reach the remaining component at all.\n",
    "\n",
    "The HNSW graph's connectivity depends on nodes that your filter removes. This isn't an edge case — it's the common case for selective filters.\n",
    "\n",
    "If you want real numbers on how many nodes you need to keep for a graph to remain connected, I highly recommend reading this short post on [HNSW & percolation theory](https://blog.vasnetsov.com/posts/categorical-hnsw/). One caveat: percolation theory assumes **random** node removal. When filter values correlate with embedding position (e.g., all products of one brand cluster together), removal is *spatially correlated* — you lose entire neighborhoods at once, which is far more destructive than the random case. The percolation thresholds in that post are optimistic for correlated filters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f112cffa",
   "metadata": {},
   "source": [
    "# 3. Qdrant's solution: filterable HNSW\n",
    "\n",
    "Not all vector databases/vector frameworks implement a solution for this problem. Many only implement pre-filtering or post-filtering.\n",
    "\n",
    "Among the providers that *do* propose a solution, the approaches differ.\n",
    "\n",
    "In this blog post, I focus on Qdrant's solution because it is simple and effective.\n",
    "\n",
    "The idea is to pre-build connected subgraphs for each filter value and merge their edges into the main graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca92a6b6",
   "metadata": {},
   "source": [
    "## How does it work\n",
    "\n",
    "Filterable HNSW adds a small set of extra edges so filtering does not disconnect matching points.\n",
    "\n",
    "Take a categorical field like `brand` with values `[Apple, Google, Sony]`. Qdrant:\n",
    "\n",
    "1. Builds a connected mini-HNSW subgraph inside each brand. That way, the products of each brand become connected together.\n",
    "2. Adds those subgraph edges to the main HNSW graph.\n",
    "\n",
    "This method guarantees that after filtering on brands, points that share the same brand will be connected (reachable from each other).\n",
    "\n",
    "It's very important to know that connectivity between *different* brands is not enforced by these subgraphs. Cross-brand connections rely on similarity links in the main graph, which may or may not survive filtering.\n",
    "\n",
    "Let us look at an example to make this clear.\n",
    "\n",
    "In the diagrams below:\n",
    "- A, B, E are Apple points\n",
    "- C, D, G are Google\n",
    "- F, H, I are Sony\n",
    "\n",
    "Qdrant builds a graph for each one of the 3 brands.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"assets/brand-subgraphs.png\" alt=\"Brand subgraphs diagram\" style=\"max-width: 100%; height: auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc3d329",
   "metadata": {},
   "source": [
    "Then these brand graphs are **merged** into the main graph. This is a key point: after the merge, there's only **one graph** in memory containing both:\n",
    "\n",
    "- **Similarity edges** (from standard HNSW construction which connects nearby vectors using similarity)\n",
    "- **Subgraph edges** (connecting nodes with the same value of the filter, e.g: brands)\n",
    "\n",
    "These aren't stored separately. The subgraph edges are simply added to the existing graph structure at build time.\n",
    "\n",
    "In the diagram below, all three brands sell similar products (earbuds, headphones, etc.), so their nodes are interleaved in embedding space. That's why the main graph also has **similarity edges across brands** including direct Apple <-> Google links."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a63e5a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"assets/merged-graph.png\" alt=\"Merged graph diagram\" style=\"max-width: 100%; height: auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a030c1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**After filtering** with `brand IN ('Apple', 'Google')`, Sony points are removed. Brand subgraph edges keep each brand internally connected. But what about the connection *between* Apple and Google?\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"assets/filtered-graph.png\" alt=\"Filtered graph diagram\" style=\"max-width: 100%; height: auto;\">\n",
    "</div>\n",
    "\n",
    "In our example, because Apple and Google sell similar products, their nodes are close in embedding space. The direct Apple <-> Google similarity edges (A–C, B–G) survived the filter, so the search can still jump between brands.\n",
    "\n",
    "But these cross-brand edges exist *because* the brands overlap in what they sell. If instead one brand only made luxury laptops and the other only budget wireless earbuds, their products would land in **different regions** of the embedding space and there would be very few or no similarity edges between them. In that case, after filtering, the search starts in one brand's cluster and never reaches the other. That's the pre-filtering failure mode all over again, just at a smaller scale.\n",
    "\n",
    "**Keep this in mind:** Filterable HNSW guarantees that **all Apple products stay connected** and **all Google products stay connected**. It does **not** guarantee that Apple products can reach Google products, this really depends on whether similarity edges bridge the gap, which depends on how correlated brand identity is with embedding position.\n",
    "\n",
    "For single-value filters (e.g., `brand = 'Apple'`), this distinction doesn't matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d44b271",
   "metadata": {},
   "source": [
    "## Does the memory explode ?\n",
    "\n",
    "The memory usage is kept well under control with filterable HNSW\n",
    "\n",
    "As each node belongs to exactly one brand,each node gains at most `m` additional edges from its brand subgraph, which makes the number of new edges at most `n * m`.\n",
    "Now, as you add more filters (not only the brand), you're going to have additional edges built."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11452ccc",
   "metadata": {},
   "source": [
    "## The query planner\n",
    "\n",
    "At query time, Qdrant estimates **filter selectivity** and decides: *should I traverse this graph, or just brute-force?*\n",
    "\n",
    "- **High or medium selectivity**: In this case, Qdrant just traverses the graph. The subgraph edges are always there and whether they're *critical* depends on how many nodes the filter removes. \n",
    " - With high selectivity (e.g, 70% match), most similarity edges still lead to valid nodes\n",
    " - With medium selectivity (e.g, 15% match), the subgraph edges become essential for maintaining connectivity.\n",
    "\n",
    "- **Low selectivity (e.g, 0.1%)**: Qdrant skips the graph entirely. When only a tiny fraction of nodes match, brute-force scanning over the filtered set is faster than graph traversal.\n",
    "\n",
    "The intuition: graph-based search only pays off when there's enough connectivity to navigate. For very selective filters, there's so little data left that exhaustive search wins.\n",
    "\n",
    "Qdrant allows you to control this behavior using `full_scan_threshold`. When the filtered candidate count falls below `full_scan_threshold`, Qdrant automatically switches to brute-force scanning.\n",
    "\n",
    "```python\n",
    "from qdrant_client.models import VectorParams, Distance, HnswConfigDiff\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"products\",\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    "    hnsw_config=HnswConfigDiff(\n",
    "        m=16,\n",
    "        ef_construct=100,\n",
    "        full_scan_threshold=1000  # switch to brute-force when fewer than 1k candidates match\n",
    "    )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff22689",
   "metadata": {},
   "source": [
    "## Getting it right in practice with Qdrant\n",
    "\n",
    "Two implementation details trip people up consistently.\n",
    "\n",
    "### 1. Create payload indexes explicitly\n",
    "\n",
    "Qdrant does not automatically index payload fields. Filtering still works without indexes — you'll get correct results — but you lose two critical optimizations:\n",
    "\n",
    "1. **No subgraph edges**: The HNSW graph won't have the extra edges needed for filtered search, so you lose the connectivity guarantees described above.\n",
    "2. **Poor cardinality estimation**: The query planner can't accurately estimate how many points match the filter, leading to suboptimal search strategies. I couldn't find the exact way it determines the strategy in the documentation but as they're also an open-source package, the curious minds can dive into their package to get this information.\n",
    "\n",
    "### 2. Create indexes before uploading data\n",
    "\n",
    "The order matters. HNSW graphs only get the subgraph edges when they're built *after* the payload index exists. If you upload data first, the HNSW graph gets built without them.\n",
    "\n",
    "If you already uploaded data without indexes, you can still fix it but you'll need to force an HNSW rebuild, which is expensive for large collections:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da610f11",
   "metadata": {},
   "source": [
    "## What about numerical filters?\n",
    "\n",
    "Everything above focused on categorical filters like `brand IN ('Apple', 'Google')`. But applications also filter on prices, ratings, timestamps, and other continuous values.\n",
    "\n",
    "Qdrant supports three numerical index types: `integer`, `float` & `datetime`\n",
    "\n",
    "### How Qdrant handles numerical ranges\n",
    "\n",
    "The [original filterable HNSW article](https://qdrant.tech/articles/filterable-hnsw/) explains the approach:\n",
    "\n",
    "> \"Numerical range case can be reduced to the previous one if we split numerical range into buckets containing equal amount of points. Next we also connect neighboring buckets to achieve graph connectivity.\"\n",
    "\n",
    "Here's how the mechanism works:\n",
    "\n",
    "1. **Buckets contain equal amounts of points** . If you have 10,000 products and 10 buckets, each bucket contains ~1,000 products regardless of how prices are distributed.\n",
    "2. **Neighboring buckets are connected** so range queries like products whose price in [150, 350]can traverse across boundaries.\n",
    "3. **Border filtering is required**: items in boundary buckets that don't match the actual filter need post-filtering. For example, with a `price < 150` filter, if bucket boundaries fall at $0, $100, $200, etc., the query would:\n",
    " - Traverse all buckets up to the one containing $150\n",
    " - Post-filter items in the boundary bucket to exclude those with price >= $150\n",
    "\n",
    "This works well in practice but isn't as clean as the solution for categorical data.\n",
    "\n",
    "And of course, you can always opt for bucketing the numerical filter data yourself as you might have a better knowledge of the data and of where the boundaries should go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd119bd",
   "metadata": {},
   "source": [
    "# 4. Conclusion\n",
    "\n",
    "There are really a couple of takeaways here:\n",
    "\n",
    "- Vector search with filters doesn't just work out of the box and is not just \"vector search + WHERE clause\n",
    "- Your Vector DB solution might not provide a solution for this, so do what any sound developer woud do, read the docs !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f75205f",
   "metadata": {},
   "source": [
    "# 5. Further reading\n",
    "\n",
    "- [Qdrant Documentation: Filtering](https://qdrant.tech/documentation/concepts/filtering/)\n",
    "- [Original Filterable HNSW Article by Qdrant](https://qdrant.tech/articles/filterable-hnsw/)\n",
    "- [HNSW Paper: Efficient and Robust Approximate Nearest Neighbor (Malkov & Yashunin, 2016)](https://arxiv.org/abs/1603.09320)\n",
    "- [Percolation Theory — Wikipedia](https://en.wikipedia.org/wiki/Percolation_theory)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
